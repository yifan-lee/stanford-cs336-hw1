{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import regex as re\n",
    "# from multiprocessing import Pool\n",
    "# from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "# from memory_profiler import profile\n",
    "# import time, tracemalloc\n",
    "# from dataclasses import dataclass\n",
    "# from typing import BinaryIO, Iterable, Iterator\n",
    "# import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, einsum, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b3d4",
   "metadata": {},
   "source": [
    "# Q 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        ## Construct a linear transformation module. This function should accept the following parameters:\n",
    "        self.in_features = in_features ## final dimension of the input\n",
    "        self.out_features = out_features ## final dimension of the output\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        w = torch.empty(in_features, out_features)\n",
    "        std = torch.sqrt(torch.tensor(2.0/(in_features+out_features)))\n",
    "        self.weight = nn.Parameter(nn.init.trunc_normal_(w, mean=0.0, std=std.item(),a=-3*std.item(),b=3*std.item()))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Apply the linear transformation to the input\n",
    "        output = einsum(\n",
    "            self.weight, x,\n",
    "            \"in_dim out_dim, in_dim -> out_dim\"\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b05a99",
   "metadata": {},
   "source": [
    "# Q 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct an embedding module\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings ## Size of the vocabulary\n",
    "        self.embedding_dim = embedding_dim ## Dimension of the embedding vectors\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "        \n",
    "        w = torch.empty(num_embeddings, embedding_dim)\n",
    "        std = 1.0\n",
    "        self.weight = nn.Parameter(nn.init.trunc_normal_(w, mean=0.0, std=std,a=-3,b=3))\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        ## Lookup the embedding vectors for the given token IDs.\n",
    "        return self.weight[token_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06587e",
   "metadata": {},
   "source": [
    "# Q 3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31176275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        ## Construct the RMSNorm module.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        self.eps = eps ## Epsilon value for numerical stability\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Process an input tensor of shape\n",
    "        \n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x_squaremean = reduce(\n",
    "            x**2, \"... d_model -> ... 1\", 'mean'\n",
    "        )\n",
    "        x_RMS = (x_squaremean+self.eps).sqrt()\n",
    "        result = x / x_RMS * self.weights\n",
    "        return result.to(in_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6984d27",
   "metadata": {},
   "source": [
    "# Q 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        if d_ff is None:\n",
    "            q = round(d_model*8/3/64)\n",
    "            self.d_ff = q*64\n",
    "        else:\n",
    "            self.d_ff = d_ff\n",
    "        \n",
    "        self.w1_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model))\n",
    "        self.w2_weight = nn.Parameter(torch.randn(self.d_model, self.d_ff))\n",
    "        self.w3_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w1x = einsum(\n",
    "            self.w1_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        w3x = einsum(\n",
    "            self.w3_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        SiLUw1x = w1x*torch.sigmoid(w1x)\n",
    "        part2 = SiLUw1x * w3x\n",
    "        result = einsum(\n",
    "            self.w2_weight, part2,\n",
    "            \"d_model d_ff, ... d_ff -> ... d_model\"\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfda749",
   "metadata": {},
   "source": [
    "# Q 3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21088daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device: torch.device | None = None):\n",
    "        ## Construct the RoPE module and create buffers if needed.\n",
    "        super().__init__()\n",
    "        assert d_k % 2 == 0, \"RoPE requires even head dimension (pairs of features)\"\n",
    "        self.theta = theta ## $\\\\Theta$ value for the RoPE\n",
    "        self.d_k = d_k ## dimension of query and key vectors\n",
    "        self.max_seq_len = max_seq_len ## Maximum sequence length that will be inputted\n",
    "        self.device = device ## Device to store the buffer on\n",
    "\n",
    "        dim_index = torch.arange(self.d_k // 2, dtype=torch.float32)\n",
    "        position_index = torch.arange(self.max_seq_len, dtype=torch.float32)\n",
    "        theta_inv_index = self.theta**(-2*dim_index/d_k)\n",
    "        theta_ik = einsum(\n",
    "            position_index, theta_inv_index,\n",
    "            \"s, d -> s d\"\n",
    "        )\n",
    "        sin = torch.sin(theta_ik)\n",
    "        cos = torch.cos(theta_ik)\n",
    "        \n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        assert x.shape[-1] == self.d_k, \"The last dim of input should be equal to dim of embedding.\"\n",
    "        assert x.shape[-2] == token_positions.shape[-1], \"token_positions length must match sequence length\"\n",
    "        sin_expend = self.sin[token_positions]\n",
    "        cos_expend = self.cos[token_positions]\n",
    "\n",
    "        x_even = x[...,::2]\n",
    "        x_odd = x[...,1::2]\n",
    "\n",
    "        y_even = x_even*cos_expend-x_odd*sin_expend\n",
    "        y_odd = x_even*sin_expend+x_odd*cos_expend\n",
    "        y = rearrange(torch.stack([y_even, y_odd], dim=-1), '... s d two -> ... s (d two)')\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e58048",
   "metadata": {},
   "source": [
    "# Q 3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b29a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim=dim, keepdim=True).values\n",
    "    x_subtract_max = x-x_max\n",
    "    x_subtract_max_exp = torch.exp(x_subtract_max)\n",
    "    x_subtract_max_exp_sum = torch.sum(x_subtract_max_exp, dim=dim, keepdim=True)\n",
    "    y = x_subtract_max_exp/x_subtract_max_exp_sum\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cff50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    mask: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    d_k = Q.shape[-1]\n",
    "    QK = einsum(\n",
    "        Q, K, \"... n d_k, ... m d_k -> ... n m\"\n",
    "    )\n",
    "    QK_scaled = QK/torch.tensor(d_k).sqrt()\n",
    "    if mask is not None:\n",
    "        M = torch.where(mask, torch.tensor(0.0), torch.tensor(float('-inf')))\n",
    "        QK_scaled += M\n",
    "    QK_scaled_softmax = softmax(QK_scaled, Q.dim()-1)\n",
    "    y = einsum(\n",
    "        QK_scaled_softmax, V, \"... n m, ... m d_v -> ... n d_v\"\n",
    "    )\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f6f3b",
   "metadata": {},
   "source": [
    "# 3.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ca0444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŒ…å«çš„ keys: ['array']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# åŠ è½½ snapshot æ–‡ä»¶\n",
    "snapshot_path = \"tests/_snapshots/test_multihead_self_attention.npz\"\n",
    "data = np.load(snapshot_path)\n",
    "\n",
    "print(\"åŒ…å«çš„ keys:\", list(data.keys()))\n",
    "correct_output = data[\"array\"]   # pytest å­˜çš„æ—¶å€™é»˜è®¤ key=\"array\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38e94cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… in_embeddings.shape: torch.Size([4, 12, 64])\n",
      "âœ… output.shape: (4, 12, 64)\n",
      "ðŸ“Š æœ€å¤§è¯¯å·®: 1.0861819\n",
      "ðŸ“Š å¹³å‡è¯¯å·®: 0.16066213\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tests.adapters import run_multihead_self_attention\n",
    "from tests.common import FIXTURES_PATH  # ä½ é¡¹ç›®é‡Œæœ‰è¿™ä¸ªè·¯å¾„å®šä¹‰\n",
    "\n",
    "# --------------------------\n",
    "# 1. å‡†å¤‡å‚æ•°ï¼ˆå’Œ conftest.py ä¸€è‡´ï¼‰\n",
    "# --------------------------\n",
    "batch_size = 4\n",
    "n_queries = 12\n",
    "n_heads = 4\n",
    "d_head = 16\n",
    "d_model = n_heads * d_head  # = 64\n",
    "\n",
    "# å›ºå®šéšæœºç§å­ï¼Œç”Ÿæˆè¾“å…¥\n",
    "torch.manual_seed(4)\n",
    "in_embeddings = torch.randn(batch_size, n_queries, d_model)\n",
    "\n",
    "# åŠ è½½æƒé‡å’Œé…ç½®\n",
    "state_dict = torch.load(FIXTURES_PATH / \"ts_tests\" / \"model.pt\", map_location=\"cpu\")\n",
    "state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "q_proj_weight = state_dict[\"layers.0.attn.q_proj.weight\"]\n",
    "k_proj_weight = state_dict[\"layers.0.attn.k_proj.weight\"]\n",
    "v_proj_weight = state_dict[\"layers.0.attn.v_proj.weight\"]\n",
    "o_proj_weight = state_dict[\"layers.0.attn.output_proj.weight\"]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2. è·‘ä½ çš„å®žçŽ°\n",
    "# --------------------------\n",
    "output = run_multihead_self_attention(\n",
    "    d_model=d_model,\n",
    "    num_heads=n_heads,\n",
    "    q_proj_weight=q_proj_weight,\n",
    "    k_proj_weight=k_proj_weight,\n",
    "    v_proj_weight=v_proj_weight,\n",
    "    o_proj_weight=o_proj_weight,\n",
    "    in_features=in_embeddings,\n",
    ")\n",
    "\n",
    "output_np = output.detach().cpu().numpy()\n",
    "\n",
    "# --------------------------\n",
    "# 3. åŠ è½½ snapshot æ ‡å‡†ç­”æ¡ˆ\n",
    "# --------------------------\n",
    "snapshot_path = Path(\"tests/_snapshots/test_multihead_self_attention.npz\")\n",
    "snapshot_output = np.load(snapshot_path)[\"array\"]\n",
    "\n",
    "# --------------------------\n",
    "# 4. å¯¹æ¯”\n",
    "# --------------------------\n",
    "diff = output_np - snapshot_output\n",
    "print(\"âœ… in_embeddings.shape:\", in_embeddings.shape)\n",
    "print(\"âœ… output.shape:\", output_np.shape)\n",
    "print(\"ðŸ“Š æœ€å¤§è¯¯å·®:\", abs(diff).max())\n",
    "print(\"ðŸ“Š å¹³å‡è¯¯å·®:\", abs(diff).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c92c5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "queries_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01e30c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17197564, -0.08255363, -0.0124781 , -0.13603452,  0.00127764,\n",
       "       -0.12001598,  0.10902623, -0.09237173, -0.02512559, -0.00557411,\n",
       "       -0.11389705, -0.20467345, -0.01746393, -0.10730196, -0.0450832 ,\n",
       "        0.00112269, -0.13262169, -0.01873078,  0.20487486,  0.05051364,\n",
       "       -0.16577597,  0.01586309,  0.3289656 ,  0.01362708, -0.1481699 ,\n",
       "        0.08887245,  0.3209791 ,  0.1910902 ,  0.14688256, -0.15865944,\n",
       "       -0.25018042, -0.08816586, -0.0465146 , -0.06768438, -0.15954879,\n",
       "        0.1471851 , -0.14069512, -0.10784942, -0.4013285 ,  0.21584041,\n",
       "       -0.20935541,  0.00850761, -0.07735043,  0.19367744, -0.10390159,\n",
       "       -0.05698189, -0.12960042, -0.12811019, -0.05007379,  0.03976776,\n",
       "        0.1602968 , -0.26809353,  0.2720321 ,  0.01819955, -0.29095235,\n",
       "       -0.04509705,  0.050052  , -0.08730943,  0.04727838, -0.03699401,\n",
       "        0.07663991, -0.14175309,  0.02848481, -0.24121457], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_np[batch_index,queries_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57d39ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24033818,  0.14196444, -0.04016975,  0.12584177,  0.2838859 ,\n",
       "        0.45506647, -0.3701429 ,  0.3658208 , -0.26944515, -0.00672541,\n",
       "       -0.40027574, -0.03942056, -0.47971496, -0.30313554,  0.02359407,\n",
       "        0.10665806,  0.5105323 ,  0.22533073, -0.3322856 , -0.05323157,\n",
       "        0.24506564, -1.0703188 ,  0.05229701, -0.5186902 ,  0.07613137,\n",
       "        0.50190294,  0.02792899, -0.16633943,  0.18831418, -0.3904159 ,\n",
       "       -0.5906472 ,  0.23029123, -0.13024016,  0.7513475 ,  0.03903922,\n",
       "        0.39151755, -0.55778044, -0.44849786,  0.26461315, -0.25431275,\n",
       "       -0.8647511 , -0.75121766,  0.37779185,  0.16481523, -0.1681257 ,\n",
       "       -0.6019287 , -0.49165186,  0.25521484, -0.63498884,  0.13234214,\n",
       "       -0.1674973 , -0.00461843, -0.03648568, -0.28829518, -0.3748052 ,\n",
       "        0.12815759,  0.429467  ,  0.07528834,  0.22757603,  0.5270285 ,\n",
       "        0.7546846 , -0.7465039 , -0.34337416, -0.19546816], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_output[batch_index,queries_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b152a9e",
   "metadata": {},
   "source": [
    "### Step1: è®©æˆ‘ä»¬ä»Žæ²¡æœ‰head å’Œ batchçš„ç®€å•æƒ…å†µå¼€å§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7d39f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç»´åº¦å‚æ•°\n",
    "batch = 1\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 1\n",
    "torch.manual_seed(4)\n",
    "# è¾“å…¥å¼ é‡\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e9a254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.3022,  3.5587,  0.0493,  2.1251],\n",
       "         [ 6.2906,  3.5453,  0.0229,  2.2065],\n",
       "         [ 6.2706,  3.5209, -0.0067,  2.2971]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q,K,V)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d2c29",
   "metadata": {},
   "source": [
    "### Step2: åŠ å…¥batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9ea4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç»´åº¦å‚æ•°\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 1\n",
    "torch.manual_seed(4)\n",
    "# è¾“å…¥å¼ é‡\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a1cfee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.4333,  -2.2128,   1.1368,  -2.7840],\n",
       "         [ -4.1671,  -0.1571,   1.0785, -19.5492],\n",
       "         [ -3.1011,  -2.1886,   1.3718,  -5.7030]],\n",
       "\n",
       "        [[  3.7037,   2.6312,  -1.1190,  13.7237],\n",
       "         [  0.0890,  -2.3049,  -0.1087,  -2.6351],\n",
       "         [  0.1030,  -2.2935,  -0.1119,  -2.5340]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q,K,V)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b8ea9",
   "metadata": {},
   "source": [
    "### Step3: åŠ å…¥head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aeb8027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç»´åº¦å‚æ•°\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# è¾“å…¥å¼ é‡\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1fc48e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9310,  6.0378, -0.0974, -0.8215],\n",
       "         [10.2525,  5.0840, -8.1525, -7.2442],\n",
       "         [-3.5506,  2.4867,  1.0241,  1.9592]],\n",
       "\n",
       "        [[-3.3017, -6.0040,  7.7340,  6.5704],\n",
       "         [ 3.6543, 11.7540,  0.8076, -8.0421],\n",
       "         [ 4.2321, -2.0617, -0.4722, -4.8180]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head,K_head,V_head)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7be6f235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4]), torch.Size([2, 3, 4]), torch.Size([2, 3, 16])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[attention.shape, in_features.shape,head.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48d2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40641e7b",
   "metadata": {},
   "source": [
    "### Step4: åŠ å…¥mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eebded8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç»´åº¦å‚æ•°\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# è¾“å…¥å¼ é‡\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.6790,  -2.4998,  -8.9383,   4.1447],\n",
       "         [ 10.3599,   5.2157,  -8.1922,  -7.3385],\n",
       "         [ -3.5506,   2.4867,   1.0241,   1.9592]],\n",
       "\n",
       "        [[-12.1002,  -3.7207,   2.2474,   7.5827],\n",
       "         [ -8.2128,   6.3416,  -2.3230,  -5.1408],\n",
       "         [  4.2321,  -2.0617,  -0.4722,  -4.8180]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "mask_boardcasted = mask.expand(expend_shape)\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head,K_head,V_head,mask_boardcasted)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9a924",
   "metadata": {},
   "source": [
    "### Step5: åŠ å…¥RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "405fca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç»´åº¦å‚æ•°\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# è¾“å…¥å¼ é‡\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b6f8918",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The last dim of input should be equal to dim of embedding.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m position_expend_shape = (Q_head.shape[:-\u001b[32m1\u001b[39m])\n\u001b[32m     22\u001b[39m position_boardcasted = position.expand(position_expend_shape)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m Q_head_rope = \u001b[43mrope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_boardcasted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m K = einsum(\n\u001b[32m     25\u001b[39m     k_proj_weight, in_features,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnd_k d_in, ... d_in -> ... nd_k\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m K_head = rearrange(\n\u001b[32m     29\u001b[39m     K, \u001b[33m\"\u001b[39m\u001b[33m... seq_len (n d_k) -> ... n seq_len d_k\u001b[39m\u001b[33m\"\u001b[39m, n = num_heads\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/stanford/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/stanford/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mRotaryPositionalEmbedding.forward\u001b[39m\u001b[34m(self, x, token_positions)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m x.shape[-\u001b[32m1\u001b[39m] == \u001b[38;5;28mself\u001b[39m.d_k, \u001b[33m\"\u001b[39m\u001b[33mThe last dim of input should be equal to dim of embedding.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m x.shape[-\u001b[32m2\u001b[39m] == token_positions.shape[-\u001b[32m1\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mtoken_positions length must match sequence length\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m     sin_expend = \u001b[38;5;28mself\u001b[39m.sin[token_positions]\n",
      "\u001b[31mAssertionError\u001b[39m: The last dim of input should be equal to dim of embedding."
     ]
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "theta = 10000\n",
    "max_seq_len = 10\n",
    "rope = RotaryPositionalEmbedding(theta, d_k, max_seq_len)\n",
    "position = torch.arange(seq_len)\n",
    "\n",
    "\n",
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "mask_boardcasted = mask.expand(expend_shape)\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "position_expend_shape = (Q_head.shape[:-1])\n",
    "position_boardcasted = position.expand(position_expend_shape)\n",
    "Q_head_rope = rope(Q_head, position_boardcasted)\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K_head_rope = rope(K_head, position_boardcasted)\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head_rope,K_head_rope,V_head,mask_boardcasted)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9565bac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 2, 3, 6]), torch.Size([2, 2, 3])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Q_head.shape, position_boardcasted.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfe79e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "baefb30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_boardcasted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9072a",
   "metadata": {},
   "source": [
    "### Step3: åŠ å…¥RoPEd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5e3003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.8476,  3.6411,  6.7984,  0.1213],\n",
       "        [ 3.6279, -1.3175, -3.8935, -0.5140],\n",
       "        [ 4.2604,  0.5462, -4.2576, -1.6795]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q_proj_weight.shape[-2]\n",
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "mask_expend_shape = (*in_features.shape[:-2], seq_len, seq_len)\n",
    "mask_boardcasted = mask.expand(mask_expend_shape)\n",
    "\n",
    "theta = 10000\n",
    "max_seq_len = 10\n",
    "rope = RotaryPositionalEmbedding(theta, d_k, max_seq_len)\n",
    "position = torch.arange(seq_len)\n",
    "position_expend_shape = (in_features.shape[:-1])\n",
    "position_boardcasted = position.expand(position_expend_shape)\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "Q_rope = rope(Q, position_boardcasted)\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K_rope = rope(K, position_boardcasted)\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_rope,K_rope,V,mask_boardcasted)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b13a4",
   "metadata": {},
   "source": [
    "### Step4: åŠ å…¥head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "id": "b82806b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç»´åº¦å‚æ•°\n",
    "batch = 1\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "\n",
    "# è¾“å…¥å¼ é‡\n",
    "# in_features = torch.randn(batch, seq_len, d_in)\n",
    "in_features = torch.randn(seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8816e920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.8476,  3.6411,  6.7984,  0.1213],\n",
       "        [ 3.6279, -1.3175, -3.8935, -0.5140],\n",
       "        [ 4.2604,  0.5462, -4.2576, -1.6795]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q_proj_weight.shape[-2]//num_heads\n",
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "mask_expend_shape = (*in_features.shape[:-2], seq_len, seq_len)\n",
    "mask_boardcasted = mask.expand(mask_expend_shape)\n",
    "\n",
    "theta = 10000\n",
    "max_seq_len = 10\n",
    "rope = RotaryPositionalEmbedding(theta, d_k*num_heads, max_seq_len)\n",
    "position = torch.arange(seq_len)\n",
    "position_expend_shape = (in_features.shape[:-1])\n",
    "position_boardcasted = position.expand(position_expend_shape)\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "Q_rope = rope(Q, position_boardcasted)\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K_rope = rope(K, position_boardcasted)\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_rope,K_rope,V,mask_boardcasted)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb21b59",
   "metadata": {},
   "source": [
    "### Step5: åŠ å…¥batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61a4d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç»´åº¦å‚æ•°\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "\n",
    "# è¾“å…¥å¼ é‡\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6041fc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-10.1454, -15.4852,  20.6310, -15.1790],\n",
       "         [ -5.7551, -12.4943,  16.7282, -12.8813],\n",
       "         [  6.5436,  -4.1160,   5.7951,  -6.4449]],\n",
       "\n",
       "        [[ 13.7090,  -1.6341,   3.2373,  -4.6695],\n",
       "         [ 13.6951,  -1.6374,   3.2414,  -4.6702],\n",
       "         [ 13.2631,  -0.5656,   4.9205,  -3.4586]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q_proj_weight.shape[-2]//num_heads\n",
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "mask_expend_shape = (*in_features.shape[:-2], seq_len, seq_len)\n",
    "mask_boardcasted = mask.expand(mask_expend_shape)\n",
    "\n",
    "theta = 10000\n",
    "max_seq_len = 10\n",
    "rope = RotaryPositionalEmbedding(theta, d_k*num_heads, max_seq_len)\n",
    "position = torch.arange(seq_len)\n",
    "position_expend_shape = (in_features.shape[:-1])\n",
    "position_boardcasted = position.expand(position_expend_shape)\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "Q_rope = rope(Q, position_boardcasted)\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K_rope = rope(K, position_boardcasted)\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_rope,K_rope,V,mask_boardcasted)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc582161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, device: torch.device | None = None):\n",
    "        super().__init__()\n",
    "        ## mplement causal multi-head self-attention\n",
    "        self.d_model = d_model ## final dimension of the input\n",
    "        self.num_heads = num_heads ## number of heads\n",
    "        self.device = device ## Device to store the parameters on\n",
    "\n",
    "        assert d_model%num_heads == 0, \"d_model/num_heads need to be int\"\n",
    "        d_k = d_model//num_heads\n",
    "        d_v = d_model//num_heads\n",
    "        \n",
    "        self.q_proj_weight = nn.Parameter(torch.randn(d_k * num_heads, d_model))\n",
    "        self.k_proj_weight = nn.Parameter(torch.randn(d_k * num_heads, d_model))\n",
    "        self.v_proj_weight = nn.Parameter(torch.randn(d_v * num_heads, d_model))\n",
    "        self.o_proj_weight = nn.Parameter(torch.randn(d_model, d_v * num_heads))\n",
    "        \n",
    "    \n",
    "    def forward(self, in_features: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        Q = einsum(\n",
    "            self.q_proj_weight, in_features,\n",
    "            \"d_k d_in, ... d_in -> ... d_k\"\n",
    "        )\n",
    "        K = einsum(\n",
    "            self.k_proj_weight, in_features,\n",
    "            \"d_k d_in, ... d_in -> ... d_k\"\n",
    "        )\n",
    "        V = einsum(\n",
    "            self.v_proj_weight, in_features,\n",
    "            \"d_v d_in, ... d_in -> ... d_v\"\n",
    "        )\n",
    "        head = scaled_dot_product_attention(Q,K,V)\n",
    "        attention = einsum(\n",
    "            head, self.o_proj_weight,\n",
    "            \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    "        )\n",
    "        return attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
