{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acb032",
   "metadata": {},
   "source": [
    "## 2.5 Experimenting with BPE Tokenizer Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d4108944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_as_str(input_path: str) -> str:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_string(string: str, special_tokens: list[str]) -> list[str]:\n",
    "    pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "    return re.split(pattern,string)\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "def get_tok_counts(string_list: list[str]) -> dict[str, int]:\n",
    "    counts = defaultdict(int)\n",
    "    for s in string_list:\n",
    "        tokens = re.finditer(PAT, s)\n",
    "        for m in tokens:\n",
    "            tok = m.group(0)\n",
    "            counts[tok] += 1\n",
    "    return counts\n",
    "\n",
    "def get_byte_counts(counts: dict[str, int])-> dict[str, int]:\n",
    "    element_counts = defaultdict(int)\n",
    "    for token, count in counts.items():\n",
    "        elements = tuple(token.encode(\"utf-8\"))\n",
    "        element_counts[elements] += count\n",
    "    return element_counts\n",
    "\n",
    "def get_pair_counts(element_counts: dict[str, int]) -> dict[tuple[str,str], int]:\n",
    "    pair_counts = defaultdict(int)\n",
    "    for elements, count in element_counts.items():\n",
    "        for i in range(len(elements)-1):\n",
    "            pair_counts[(elements[i],elements[i+1])] += count\n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "def update_element_counts(byte_level_counts: dict[str, int], pair: tuple[str, str], new_index: int) -> dict[str, int]:\n",
    "    new_byte_level_counts = {}\n",
    "    for elements, counts in byte_level_counts.items():\n",
    "        new_element = []\n",
    "        elements_len = len(elements)\n",
    "        index = 0\n",
    "        while index <= elements_len-1:\n",
    "            if (index < elements_len-1) and (elements[index] == pair[0]) and (elements[index+1] == pair[1]):\n",
    "                new_element.append(new_index)\n",
    "                index += 2\n",
    "            else:\n",
    "                new_element.append(elements[index])\n",
    "                index += 1\n",
    "        new_byte_level_counts[tuple(new_element)] = counts\n",
    "    return new_byte_level_counts  \n",
    "\n",
    "def initiate_vocab(special_tokens: list[str]) ->  dict[int, bytes]:\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    for i, tok in enumerate(special_tokens, start=256):\n",
    "        vocab[i] = tok.encode(\"utf-8\")\n",
    "    return vocab   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a82139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize(string: str,vocab_size: int,special_tokens: list[str]) -> tuple[dict[int, bytes],list[tuple[bytes, bytes]]]:\n",
    "    merges = []\n",
    "    string_list = split_string(string, special_tokens)\n",
    "    word_level_counts = get_tok_counts(string_list)\n",
    "    byte_level_counts = get_byte_counts(word_level_counts)\n",
    "    vocab = initiate_vocab(special_tokens)\n",
    "    vocab_len = len(vocab)\n",
    "\n",
    "    while vocab_len<vocab_size:\n",
    "        pair_counts = get_pair_counts(byte_level_counts)\n",
    "        while len(pair_counts) == 0:\n",
    "            break\n",
    "        pair = max(pair_counts, key=lambda k: (pair_counts[k], k))\n",
    "        index1, index2 = pair\n",
    "        new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "        new_index = vocab_len\n",
    "        byte_level_counts = update_element_counts(byte_level_counts, pair,new_index)\n",
    "        merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "        vocab[new_index] = new_token\n",
    "        vocab_len+=1\n",
    "    return vocab, merges\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    string = load_txt_as_str(input_path)\n",
    "    vocab, merges = pre_tokenize(string, vocab_size,special_tokens)\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "4e4f25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'\n",
    "\n",
    "epochs = 6\n",
    "vocab_size = 300\n",
    "string = \"hi. i'm yifan li. nice to meet you.<|endoftext|> this the what when here where\"\n",
    "# input_path = r'./data/test.txt'\n",
    "# string = load_txt_as_str(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4115842",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenize(string,vocab_size,special_tokens: list[str]) -> tuple[dict[int, bytes],list[tuple[bytes, bytes]]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9beef343",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() iterable argument is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[347]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m vocab_len<vocab_size:\n\u001b[32m     10\u001b[39m     pair_counts = get_pair_counts(byte_level_counts)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     pair = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpair_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     index1, index2 = pair\n\u001b[32m     13\u001b[39m     new_token = vocab[\u001b[38;5;28mint\u001b[39m(index1)]+vocab[\u001b[38;5;28mint\u001b[39m(index2)]\n",
      "\u001b[31mValueError\u001b[39m: max() iterable argument is empty"
     ]
    }
   ],
   "source": [
    "merges = []\n",
    "string_list = split_string(string, special_tokens)\n",
    "word_level_counts = get_tok_counts(string_list)\n",
    "byte_level_counts = get_byte_counts(word_level_counts)\n",
    "vocab = initiate_vocab(special_tokens)\n",
    "special_tokens_len = len(special_tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "while vocab_len<vocab_size:\n",
    "    pair_counts = get_pair_counts(byte_level_counts)\n",
    "    pair = max(pair_counts, key=lambda k: (pair_counts[k], k))\n",
    "    index1, index2 = pair\n",
    "    new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "    new_index = vocab_len\n",
    "    byte_level_counts = update_element_counts(byte_level_counts, pair,new_index)\n",
    "    merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "    vocab[new_index] = new_token\n",
    "    vocab_len+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "8c023152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6a428641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(104, 105): 1,\n",
       " (46,): 3,\n",
       " (32, 105): 1,\n",
       " (39, 109): 1,\n",
       " (32, 121, 105, 102, 97, 110): 1,\n",
       " (32, 108, 105): 1,\n",
       " (32, 110, 105, 99, 101): 1,\n",
       " (32, 116, 111): 1,\n",
       " (32, 109, 101, 101, 116): 1,\n",
       " (32, 121, 111, 117): 1,\n",
       " (32, 116, 104, 105, 115): 1,\n",
       " (32, 116, 257): 1,\n",
       " (32, 119, 104, 97, 116): 1,\n",
       " (32, 119, 257, 110): 1,\n",
       " (32, 257, 114, 101): 1,\n",
       " (32, 119, 257, 114, 101): 1}"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_level_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3b210bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'he'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3c16fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_byte_counts(counts: dict[str, int])-> dict[str, int]:\n",
    "    element_counts = defaultdict(int)\n",
    "    for token, count in counts.items():\n",
    "        elements = tuple(token.encode(\"utf-8\"))\n",
    "        element_counts[elements] += count\n",
    "    return element_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "92dc3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "b833e155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'<|endoftext|>',\n",
       " 1: b'\\x00',\n",
       " 2: b'\\x01',\n",
       " 3: b'\\x02',\n",
       " 4: b'\\x03',\n",
       " 5: b'\\x04',\n",
       " 6: b'\\x05',\n",
       " 7: b'\\x06',\n",
       " 8: b'\\x07',\n",
       " 9: b'\\x08',\n",
       " 10: b'\\t',\n",
       " 11: b'\\n',\n",
       " 12: b'\\x0b',\n",
       " 13: b'\\x0c',\n",
       " 14: b'\\r',\n",
       " 15: b'\\x0e',\n",
       " 16: b'\\x0f',\n",
       " 17: b'\\x10',\n",
       " 18: b'\\x11',\n",
       " 19: b'\\x12',\n",
       " 20: b'\\x13',\n",
       " 21: b'\\x14',\n",
       " 22: b'\\x15',\n",
       " 23: b'\\x16',\n",
       " 24: b'\\x17',\n",
       " 25: b'\\x18',\n",
       " 26: b'\\x19',\n",
       " 27: b'\\x1a',\n",
       " 28: b'\\x1b',\n",
       " 29: b'\\x1c',\n",
       " 30: b'\\x1d',\n",
       " 31: b'\\x1e',\n",
       " 32: b'\\x1f',\n",
       " 33: b' ',\n",
       " 34: b'!',\n",
       " 35: b'\"',\n",
       " 36: b'#',\n",
       " 37: b'$',\n",
       " 38: b'%',\n",
       " 39: b'&',\n",
       " 40: b\"'\",\n",
       " 41: b'(',\n",
       " 42: b')',\n",
       " 43: b'*',\n",
       " 44: b'+',\n",
       " 45: b',',\n",
       " 46: b'-',\n",
       " 47: b'.',\n",
       " 48: b'/',\n",
       " 49: b'0',\n",
       " 50: b'1',\n",
       " 51: b'2',\n",
       " 52: b'3',\n",
       " 53: b'4',\n",
       " 54: b'5',\n",
       " 55: b'6',\n",
       " 56: b'7',\n",
       " 57: b'8',\n",
       " 58: b'9',\n",
       " 59: b':',\n",
       " 60: b';',\n",
       " 61: b'<',\n",
       " 62: b'=',\n",
       " 63: b'>',\n",
       " 64: b'?',\n",
       " 65: b'@',\n",
       " 66: b'A',\n",
       " 67: b'B',\n",
       " 68: b'C',\n",
       " 69: b'D',\n",
       " 70: b'E',\n",
       " 71: b'F',\n",
       " 72: b'G',\n",
       " 73: b'H',\n",
       " 74: b'I',\n",
       " 75: b'J',\n",
       " 76: b'K',\n",
       " 77: b'L',\n",
       " 78: b'M',\n",
       " 79: b'N',\n",
       " 80: b'O',\n",
       " 81: b'P',\n",
       " 82: b'Q',\n",
       " 83: b'R',\n",
       " 84: b'S',\n",
       " 85: b'T',\n",
       " 86: b'U',\n",
       " 87: b'V',\n",
       " 88: b'W',\n",
       " 89: b'X',\n",
       " 90: b'Y',\n",
       " 91: b'Z',\n",
       " 92: b'[',\n",
       " 93: b'\\\\',\n",
       " 94: b']',\n",
       " 95: b'^',\n",
       " 96: b'_',\n",
       " 97: b'`',\n",
       " 98: b'a',\n",
       " 99: b'b',\n",
       " 100: b'c',\n",
       " 101: b'd',\n",
       " 102: b'e',\n",
       " 103: b'f',\n",
       " 104: b'g',\n",
       " 105: b'h',\n",
       " 106: b'i',\n",
       " 107: b'j',\n",
       " 108: b'k',\n",
       " 109: b'l',\n",
       " 110: b'm',\n",
       " 111: b'n',\n",
       " 112: b'o',\n",
       " 113: b'p',\n",
       " 114: b'q',\n",
       " 115: b'r',\n",
       " 116: b's',\n",
       " 117: b't',\n",
       " 118: b'u',\n",
       " 119: b'v',\n",
       " 120: b'w',\n",
       " 121: b'x',\n",
       " 122: b'y',\n",
       " 123: b'z',\n",
       " 124: b'{',\n",
       " 125: b'|',\n",
       " 126: b'}',\n",
       " 127: b'~',\n",
       " 128: b'\\x7f',\n",
       " 129: b'\\x80',\n",
       " 130: b'\\x81',\n",
       " 131: b'\\x82',\n",
       " 132: b'\\x83',\n",
       " 133: b'\\x84',\n",
       " 134: b'\\x85',\n",
       " 135: b'\\x86',\n",
       " 136: b'\\x87',\n",
       " 137: b'\\x88',\n",
       " 138: b'\\x89',\n",
       " 139: b'\\x8a',\n",
       " 140: b'\\x8b',\n",
       " 141: b'\\x8c',\n",
       " 142: b'\\x8d',\n",
       " 143: b'\\x8e',\n",
       " 144: b'\\x8f',\n",
       " 145: b'\\x90',\n",
       " 146: b'\\x91',\n",
       " 147: b'\\x92',\n",
       " 148: b'\\x93',\n",
       " 149: b'\\x94',\n",
       " 150: b'\\x95',\n",
       " 151: b'\\x96',\n",
       " 152: b'\\x97',\n",
       " 153: b'\\x98',\n",
       " 154: b'\\x99',\n",
       " 155: b'\\x9a',\n",
       " 156: b'\\x9b',\n",
       " 157: b'\\x9c',\n",
       " 158: b'\\x9d',\n",
       " 159: b'\\x9e',\n",
       " 160: b'\\x9f',\n",
       " 161: b'\\xa0',\n",
       " 162: b'\\xa1',\n",
       " 163: b'\\xa2',\n",
       " 164: b'\\xa3',\n",
       " 165: b'\\xa4',\n",
       " 166: b'\\xa5',\n",
       " 167: b'\\xa6',\n",
       " 168: b'\\xa7',\n",
       " 169: b'\\xa8',\n",
       " 170: b'\\xa9',\n",
       " 171: b'\\xaa',\n",
       " 172: b'\\xab',\n",
       " 173: b'\\xac',\n",
       " 174: b'\\xad',\n",
       " 175: b'\\xae',\n",
       " 176: b'\\xaf',\n",
       " 177: b'\\xb0',\n",
       " 178: b'\\xb1',\n",
       " 179: b'\\xb2',\n",
       " 180: b'\\xb3',\n",
       " 181: b'\\xb4',\n",
       " 182: b'\\xb5',\n",
       " 183: b'\\xb6',\n",
       " 184: b'\\xb7',\n",
       " 185: b'\\xb8',\n",
       " 186: b'\\xb9',\n",
       " 187: b'\\xba',\n",
       " 188: b'\\xbb',\n",
       " 189: b'\\xbc',\n",
       " 190: b'\\xbd',\n",
       " 191: b'\\xbe',\n",
       " 192: b'\\xbf',\n",
       " 193: b'\\xc0',\n",
       " 194: b'\\xc1',\n",
       " 195: b'\\xc2',\n",
       " 196: b'\\xc3',\n",
       " 197: b'\\xc4',\n",
       " 198: b'\\xc5',\n",
       " 199: b'\\xc6',\n",
       " 200: b'\\xc7',\n",
       " 201: b'\\xc8',\n",
       " 202: b'\\xc9',\n",
       " 203: b'\\xca',\n",
       " 204: b'\\xcb',\n",
       " 205: b'\\xcc',\n",
       " 206: b'\\xcd',\n",
       " 207: b'\\xce',\n",
       " 208: b'\\xcf',\n",
       " 209: b'\\xd0',\n",
       " 210: b'\\xd1',\n",
       " 211: b'\\xd2',\n",
       " 212: b'\\xd3',\n",
       " 213: b'\\xd4',\n",
       " 214: b'\\xd5',\n",
       " 215: b'\\xd6',\n",
       " 216: b'\\xd7',\n",
       " 217: b'\\xd8',\n",
       " 218: b'\\xd9',\n",
       " 219: b'\\xda',\n",
       " 220: b'\\xdb',\n",
       " 221: b'\\xdc',\n",
       " 222: b'\\xdd',\n",
       " 223: b'\\xde',\n",
       " 224: b'\\xdf',\n",
       " 225: b'\\xe0',\n",
       " 226: b'\\xe1',\n",
       " 227: b'\\xe2',\n",
       " 228: b'\\xe3',\n",
       " 229: b'\\xe4',\n",
       " 230: b'\\xe5',\n",
       " 231: b'\\xe6',\n",
       " 232: b'\\xe7',\n",
       " 233: b'\\xe8',\n",
       " 234: b'\\xe9',\n",
       " 235: b'\\xea',\n",
       " 236: b'\\xeb',\n",
       " 237: b'\\xec',\n",
       " 238: b'\\xed',\n",
       " 239: b'\\xee',\n",
       " 240: b'\\xef',\n",
       " 241: b'\\xf0',\n",
       " 242: b'\\xf1',\n",
       " 243: b'\\xf2',\n",
       " 244: b'\\xf3',\n",
       " 245: b'\\xf4',\n",
       " 246: b'\\xf5',\n",
       " 247: b'\\xf6',\n",
       " 248: b'\\xf7',\n",
       " 249: b'\\xf8',\n",
       " 250: b'\\xf9',\n",
       " 251: b'\\xfa',\n",
       " 252: b'\\xfb',\n",
       " 253: b'\\xfc',\n",
       " 254: b'\\xfd',\n",
       " 255: b'\\xfe',\n",
       " 256: b'\\xff',\n",
       " 257: b'gd',\n",
       " 258: b'\\x1fs',\n",
       " 259: b'\\x1fsgd',\n",
       " 260: b'\\x1fr',\n",
       " 261: b'mc',\n",
       " 262: b'\\x1fa',\n",
       " 263: b'\\x1f`',\n",
       " 264: b'\\x1fS',\n",
       " 265: b'dc',\n",
       " 266: b'\\x1fv',\n",
       " 267: b'hs',\n",
       " 268: b'`q',\n",
       " 269: b'\\x1fo',\n",
       " 270: b'\\x1fsn',\n",
       " 271: b'\\x1fe',\n",
       " 272: b'\\x1f`mc',\n",
       " 273: b'kk',\n",
       " 274: b'\\x1fohs',\n",
       " 275: b'\\x1fSgd',\n",
       " 276: b'\\x1fk',\n",
       " 277: b'nt',\n",
       " 278: b'hm',\n",
       " 279: b'`kk',\n",
       " 280: b'\\x1fg',\n",
       " 281: b'\\x1fSgdx',\n",
       " 282: b'\\x1fv`',\n",
       " 283: b'\\x1fa`kk',\n",
       " 284: b'nl',\n",
       " 285: b'\\x1fv`r',\n",
       " 286: b'\\x1fgh',\n",
       " 287: b'\\x1fSnl',\n",
       " 288: b'\\x1fr`',\n",
       " 289: b'dq',\n",
       " 290: b'\\x1fhm',\n",
       " 291: b'\\x1fm',\n",
       " 292: b'\\x1fl',\n",
       " 293: b'\\x1fc',\n",
       " 294: b'\\x1fb',\n",
       " 295: b'\\x1fat',\n",
       " 296: b'\\x1fats',\n",
       " 297: b'rs',\n",
       " 298: b'kd',\n",
       " 299: b'`l'}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "f386f7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'g', b'd'),\n",
       " (b'\\x1f', b's'),\n",
       " (b'\\x1fs', b'gd'),\n",
       " (b'\\x1f', b'r'),\n",
       " (b'm', b'c'),\n",
       " (b'\\x1f', b'a'),\n",
       " (b'\\x1f', b'`'),\n",
       " (b'\\x1f', b'S'),\n",
       " (b'd', b'c'),\n",
       " (b'\\x1f', b'v'),\n",
       " (b'h', b's'),\n",
       " (b'`', b'q'),\n",
       " (b'\\x1f', b'o'),\n",
       " (b'\\x1fs', b'n'),\n",
       " (b'\\x1f', b'e'),\n",
       " (b'\\x1f`', b'mc'),\n",
       " (b'k', b'k'),\n",
       " (b'\\x1fo', b'hs'),\n",
       " (b'\\x1fS', b'gd'),\n",
       " (b'\\x1f', b'k'),\n",
       " (b'n', b't'),\n",
       " (b'h', b'm'),\n",
       " (b'`', b'kk'),\n",
       " (b'\\x1f', b'g'),\n",
       " (b'\\x1fSgd', b'x'),\n",
       " (b'\\x1fv', b'`'),\n",
       " (b'\\x1fa', b'`kk'),\n",
       " (b'n', b'l'),\n",
       " (b'\\x1fv`', b'r'),\n",
       " (b'\\x1fg', b'h'),\n",
       " (b'\\x1fS', b'nl'),\n",
       " (b'\\x1fr', b'`'),\n",
       " (b'd', b'q'),\n",
       " (b'\\x1f', b'hm'),\n",
       " (b'\\x1f', b'm'),\n",
       " (b'\\x1f', b'l'),\n",
       " (b'\\x1f', b'c'),\n",
       " (b'\\x1f', b'b'),\n",
       " (b'\\x1fa', b't'),\n",
       " (b'\\x1fat', b's'),\n",
       " (b'r', b's'),\n",
       " (b'k', b'd'),\n",
       " (b'`', b'l')]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ed6a6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_level_counts = byte_level_counts\n",
    "pair = pair\n",
    "new_index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "99a13cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_byte_level_counts = {}\n",
    "for elements, counts in byte_level_counts.items():\n",
    "    new_element = []\n",
    "    elements_len = len(elements)\n",
    "    index = 0\n",
    "    while index <= elements_len-1:\n",
    "        if (index < elements_len-1) and (elements[index] == pair[0]) and (elements[index+1] == pair[1]):\n",
    "            new_element.append(new_index)\n",
    "            index += 2\n",
    "        else:\n",
    "            new_element.append(elements[index])\n",
    "            index += 1\n",
    "    new_byte_level_counts[tuple(new_element)] = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8adeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'<|endoftext|>',\n",
       " 1: b'\\x00',\n",
       " 2: b'\\x01',\n",
       " 3: b'\\x02',\n",
       " 4: b'\\x03',\n",
       " 5: b'\\x04',\n",
       " 6: b'\\x05',\n",
       " 7: b'\\x06',\n",
       " 8: b'\\x07',\n",
       " 9: b'\\x08',\n",
       " 10: b'\\t',\n",
       " 11: b'\\n',\n",
       " 12: b'\\x0b',\n",
       " 13: b'\\x0c',\n",
       " 14: b'\\r',\n",
       " 15: b'\\x0e',\n",
       " 16: b'\\x0f',\n",
       " 17: b'\\x10',\n",
       " 18: b'\\x11',\n",
       " 19: b'\\x12',\n",
       " 20: b'\\x13',\n",
       " 21: b'\\x14',\n",
       " 22: b'\\x15',\n",
       " 23: b'\\x16',\n",
       " 24: b'\\x17',\n",
       " 25: b'\\x18',\n",
       " 26: b'\\x19',\n",
       " 27: b'\\x1a',\n",
       " 28: b'\\x1b',\n",
       " 29: b'\\x1c',\n",
       " 30: b'\\x1d',\n",
       " 31: b'\\x1e',\n",
       " 32: b'\\x1f',\n",
       " 33: b' ',\n",
       " 34: b'!',\n",
       " 35: b'\"',\n",
       " 36: b'#',\n",
       " 37: b'$',\n",
       " 38: b'%',\n",
       " 39: b'&',\n",
       " 40: b\"'\",\n",
       " 41: b'(',\n",
       " 42: b')',\n",
       " 43: b'*',\n",
       " 44: b'+',\n",
       " 45: b',',\n",
       " 46: b'-',\n",
       " 47: b'.',\n",
       " 48: b'/',\n",
       " 49: b'0',\n",
       " 50: b'1',\n",
       " 51: b'2',\n",
       " 52: b'3',\n",
       " 53: b'4',\n",
       " 54: b'5',\n",
       " 55: b'6',\n",
       " 56: b'7',\n",
       " 57: b'8',\n",
       " 58: b'9',\n",
       " 59: b':',\n",
       " 60: b';',\n",
       " 61: b'<',\n",
       " 62: b'=',\n",
       " 63: b'>',\n",
       " 64: b'?',\n",
       " 65: b'@',\n",
       " 66: b'A',\n",
       " 67: b'B',\n",
       " 68: b'C',\n",
       " 69: b'D',\n",
       " 70: b'E',\n",
       " 71: b'F',\n",
       " 72: b'G',\n",
       " 73: b'H',\n",
       " 74: b'I',\n",
       " 75: b'J',\n",
       " 76: b'K',\n",
       " 77: b'L',\n",
       " 78: b'M',\n",
       " 79: b'N',\n",
       " 80: b'O',\n",
       " 81: b'P',\n",
       " 82: b'Q',\n",
       " 83: b'R',\n",
       " 84: b'S',\n",
       " 85: b'T',\n",
       " 86: b'U',\n",
       " 87: b'V',\n",
       " 88: b'W',\n",
       " 89: b'X',\n",
       " 90: b'Y',\n",
       " 91: b'Z',\n",
       " 92: b'[',\n",
       " 93: b'\\\\',\n",
       " 94: b']',\n",
       " 95: b'^',\n",
       " 96: b'_',\n",
       " 97: b'`',\n",
       " 98: b'a',\n",
       " 99: b'b',\n",
       " 100: b'c',\n",
       " 101: b'd',\n",
       " 102: b'e',\n",
       " 103: b'f',\n",
       " 104: b'g',\n",
       " 105: b'h',\n",
       " 106: b'i',\n",
       " 107: b'j',\n",
       " 108: b'k',\n",
       " 109: b'l',\n",
       " 110: b'm',\n",
       " 111: b'n',\n",
       " 112: b'o',\n",
       " 113: b'p',\n",
       " 114: b'q',\n",
       " 115: b'r',\n",
       " 116: b's',\n",
       " 117: b't',\n",
       " 118: b'u',\n",
       " 119: b'v',\n",
       " 120: b'w',\n",
       " 121: b'x',\n",
       " 122: b'y',\n",
       " 123: b'z',\n",
       " 124: b'{',\n",
       " 125: b'|',\n",
       " 126: b'}',\n",
       " 127: b'~',\n",
       " 128: b'\\x7f',\n",
       " 129: b'\\x80',\n",
       " 130: b'\\x81',\n",
       " 131: b'\\x82',\n",
       " 132: b'\\x83',\n",
       " 133: b'\\x84',\n",
       " 134: b'\\x85',\n",
       " 135: b'\\x86',\n",
       " 136: b'\\x87',\n",
       " 137: b'\\x88',\n",
       " 138: b'\\x89',\n",
       " 139: b'\\x8a',\n",
       " 140: b'\\x8b',\n",
       " 141: b'\\x8c',\n",
       " 142: b'\\x8d',\n",
       " 143: b'\\x8e',\n",
       " 144: b'\\x8f',\n",
       " 145: b'\\x90',\n",
       " 146: b'\\x91',\n",
       " 147: b'\\x92',\n",
       " 148: b'\\x93',\n",
       " 149: b'\\x94',\n",
       " 150: b'\\x95',\n",
       " 151: b'\\x96',\n",
       " 152: b'\\x97',\n",
       " 153: b'\\x98',\n",
       " 154: b'\\x99',\n",
       " 155: b'\\x9a',\n",
       " 156: b'\\x9b',\n",
       " 157: b'\\x9c',\n",
       " 158: b'\\x9d',\n",
       " 159: b'\\x9e',\n",
       " 160: b'\\x9f',\n",
       " 161: b'\\xa0',\n",
       " 162: b'\\xa1',\n",
       " 163: b'\\xa2',\n",
       " 164: b'\\xa3',\n",
       " 165: b'\\xa4',\n",
       " 166: b'\\xa5',\n",
       " 167: b'\\xa6',\n",
       " 168: b'\\xa7',\n",
       " 169: b'\\xa8',\n",
       " 170: b'\\xa9',\n",
       " 171: b'\\xaa',\n",
       " 172: b'\\xab',\n",
       " 173: b'\\xac',\n",
       " 174: b'\\xad',\n",
       " 175: b'\\xae',\n",
       " 176: b'\\xaf',\n",
       " 177: b'\\xb0',\n",
       " 178: b'\\xb1',\n",
       " 179: b'\\xb2',\n",
       " 180: b'\\xb3',\n",
       " 181: b'\\xb4',\n",
       " 182: b'\\xb5',\n",
       " 183: b'\\xb6',\n",
       " 184: b'\\xb7',\n",
       " 185: b'\\xb8',\n",
       " 186: b'\\xb9',\n",
       " 187: b'\\xba',\n",
       " 188: b'\\xbb',\n",
       " 189: b'\\xbc',\n",
       " 190: b'\\xbd',\n",
       " 191: b'\\xbe',\n",
       " 192: b'\\xbf',\n",
       " 193: b'\\xc0',\n",
       " 194: b'\\xc1',\n",
       " 195: b'\\xc2',\n",
       " 196: b'\\xc3',\n",
       " 197: b'\\xc4',\n",
       " 198: b'\\xc5',\n",
       " 199: b'\\xc6',\n",
       " 200: b'\\xc7',\n",
       " 201: b'\\xc8',\n",
       " 202: b'\\xc9',\n",
       " 203: b'\\xca',\n",
       " 204: b'\\xcb',\n",
       " 205: b'\\xcc',\n",
       " 206: b'\\xcd',\n",
       " 207: b'\\xce',\n",
       " 208: b'\\xcf',\n",
       " 209: b'\\xd0',\n",
       " 210: b'\\xd1',\n",
       " 211: b'\\xd2',\n",
       " 212: b'\\xd3',\n",
       " 213: b'\\xd4',\n",
       " 214: b'\\xd5',\n",
       " 215: b'\\xd6',\n",
       " 216: b'\\xd7',\n",
       " 217: b'\\xd8',\n",
       " 218: b'\\xd9',\n",
       " 219: b'\\xda',\n",
       " 220: b'\\xdb',\n",
       " 221: b'\\xdc',\n",
       " 222: b'\\xdd',\n",
       " 223: b'\\xde',\n",
       " 224: b'\\xdf',\n",
       " 225: b'\\xe0',\n",
       " 226: b'\\xe1',\n",
       " 227: b'\\xe2',\n",
       " 228: b'\\xe3',\n",
       " 229: b'\\xe4',\n",
       " 230: b'\\xe5',\n",
       " 231: b'\\xe6',\n",
       " 232: b'\\xe7',\n",
       " 233: b'\\xe8',\n",
       " 234: b'\\xe9',\n",
       " 235: b'\\xea',\n",
       " 236: b'\\xeb',\n",
       " 237: b'\\xec',\n",
       " 238: b'\\xed',\n",
       " 239: b'\\xee',\n",
       " 240: b'\\xef',\n",
       " 241: b'\\xf0',\n",
       " 242: b'\\xf1',\n",
       " 243: b'\\xf2',\n",
       " 244: b'\\xf3',\n",
       " 245: b'\\xf4',\n",
       " 246: b'\\xf5',\n",
       " 247: b'\\xf6',\n",
       " 248: b'\\xf7',\n",
       " 249: b'\\xf8',\n",
       " 250: b'\\xf9',\n",
       " 251: b'\\xfa',\n",
       " 252: b'\\xfb',\n",
       " 253: b'\\xfc',\n",
       " 254: b'\\xfd',\n",
       " 255: b'\\xfe',\n",
       " 256: b'\\xff',\n",
       " 257: b'gd',\n",
       " 258: b'gd',\n",
       " 259: b'gd',\n",
       " 260: b'gd',\n",
       " 261: b'gd',\n",
       " 262: b'gd',\n",
       " 263: b'gd',\n",
       " 264: b'gd',\n",
       " 265: b'gd',\n",
       " 266: b'gd',\n",
       " 267: b'gd',\n",
       " 268: b'gd',\n",
       " 269: b'gd',\n",
       " 270: b'gd',\n",
       " 271: b'gd',\n",
       " 272: b'gd',\n",
       " 273: b'gd',\n",
       " 274: b'gd',\n",
       " 275: b'gd',\n",
       " 276: b'gd',\n",
       " 277: b'gd',\n",
       " 278: b'gd',\n",
       " 279: b'gd',\n",
       " 280: b'gd',\n",
       " 281: b'gd',\n",
       " 282: b'gd',\n",
       " 283: b'gd',\n",
       " 284: b'gd',\n",
       " 285: b'gd',\n",
       " 286: b'gd',\n",
       " 287: b'gd',\n",
       " 288: b'gd',\n",
       " 289: b'gd',\n",
       " 290: b'gd',\n",
       " 291: b'gd',\n",
       " 292: b'gd',\n",
       " 293: b'gd',\n",
       " 294: b'gd',\n",
       " 295: b'gd',\n",
       " 296: b'gd',\n",
       " 297: b'gd',\n",
       " 298: b'gd',\n",
       " 299: b'gd'}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f7b3049b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'<|endoftext|>', 2: b'\\x00', 3: b'\\x02'}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "vocab[i+special_tokens_len] = bytes([i])\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e122199",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab: dict[int, bytes]  = {i:bytes([i]) for i in range(256)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "330713b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[185]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m FIXTURES_PATH = (pathlib.Path(\u001b[34;43m__file__\u001b[39;49m).resolve().parent) / \u001b[33m\"\u001b[39m\u001b[33mfixtures\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "FIXTURES_PATH = (pathlib.Path(__file__).resolve().parent) / \"fixtures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c5c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
