{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import regex as re\n",
    "# from multiprocessing import Pool\n",
    "# from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "# from memory_profiler import profile\n",
    "# import time, tracemalloc\n",
    "# from dataclasses import dataclass\n",
    "# from typing import BinaryIO, Iterable, Iterator\n",
    "# import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, einsum, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b3d4",
   "metadata": {},
   "source": [
    "# Q 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        ## Construct a linear transformation module. This function should accept the following parameters:\n",
    "        self.in_features = in_features ## final dimension of the input\n",
    "        self.out_features = out_features ## final dimension of the output\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.empty(out_features, in_features, device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        std = torch.sqrt(torch.tensor(2.0/(in_features+out_features)))\n",
    "        nn.init.trunc_normal_(self.weights, mean=0.0, std=std.item(), a=-3*std.item(), b=3*std.item())\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Apply the linear transformation to the input\n",
    "        output = einsum(\n",
    "            x, self.weights,\n",
    "            \"... in_dim, out_dim in_dim -> ... out_dim\"\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b05a99",
   "metadata": {},
   "source": [
    "# Q 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct an embedding module\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings ## Size of the vocabulary\n",
    "        self.embedding_dim = embedding_dim ## Dimension of the embedding vectors\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "        \n",
    "        self.weights = nn.Parameter(\n",
    "            torch.empty(num_embeddings, embedding_dim, device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        std = 1.0\n",
    "        nn.init.trunc_normal_(self.weights, mean=0.0, std=std,a=-3,b=3)\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        ## Lookup the embedding vectors for the given token IDs.\n",
    "        return self.weights[token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06587e",
   "metadata": {},
   "source": [
    "# Q 3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31176275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct the RMSNorm module.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        self.eps = eps ## Epsilon value for numerical stability\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(d_model, device=self.device, dtype=self.dtype))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Process an input tensor of shape\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x_squaremean = reduce(\n",
    "            x**2, \"... d_model -> ... 1\", 'mean'\n",
    "        )\n",
    "        x_RMS = (x_squaremean+self.eps).sqrt()\n",
    "        result = x / x_RMS * self.weights\n",
    "        return result.to(in_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6984d27",
   "metadata": {},
   "source": [
    "# Q 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        if d_ff is None:\n",
    "            q = round(d_model*8/3/64)\n",
    "            self.d_ff = q*64\n",
    "        else:\n",
    "            self.d_ff = d_ff\n",
    "        \n",
    "        self.w1_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model, device=self.device, dtype=self.dtype))\n",
    "        self.w2_weight = nn.Parameter(torch.randn(self.d_model, self.d_ff, device=self.device, dtype=self.dtype))\n",
    "        self.w3_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model, device=self.device, dtype=self.dtype))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w1x = einsum(\n",
    "            self.w1_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        w3x = einsum(\n",
    "            self.w3_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        SiLUw1x = w1x*torch.sigmoid(w1x)\n",
    "        part2 = SiLUw1x * w3x\n",
    "        result = einsum(\n",
    "            self.w2_weight, part2,\n",
    "            \"d_model d_ff, ... d_ff -> ... d_model\"\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfda749",
   "metadata": {},
   "source": [
    "# Q 3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21088daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct the RoPE module and create buffers if needed.\n",
    "        super().__init__()\n",
    "        assert d_k % 2 == 0, \"RoPE requires even head dimension (pairs of features)\"\n",
    "        self.theta = theta ## $\\\\Theta$ value for the RoPE\n",
    "        self.d_k = d_k ## dimension of query and key vectors\n",
    "        self.max_seq_len = max_seq_len ## Maximum sequence length that will be inputted\n",
    "        self.device = device ## Device to store the buffer on\n",
    "\n",
    "        dim_index = torch.arange(self.d_k // 2, device=self.device, dtype=torch.float32)\n",
    "        position_index = torch.arange(self.max_seq_len, device=self.device, dtype=torch.float32)\n",
    "        theta_inv_index = self.theta**(-2*dim_index/d_k)\n",
    "        theta_ik = einsum(\n",
    "            position_index, theta_inv_index,\n",
    "            \"s, d -> s d\"\n",
    "        )\n",
    "        sin = torch.sin(theta_ik)\n",
    "        cos = torch.cos(theta_ik)\n",
    "        \n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        assert x.shape[-1] == self.d_k, \"The last dim of input should be equal to dim of embedding.\"\n",
    "        assert x.shape[-2] == token_positions.shape[-1], \"token_positions length must match sequence length\"\n",
    "        sin_expend = self.sin[token_positions]\n",
    "        cos_expend = self.cos[token_positions]\n",
    "\n",
    "        x_even = x[...,::2]\n",
    "        x_odd = x[...,1::2]\n",
    "\n",
    "        y_even = x_even*cos_expend-x_odd*sin_expend\n",
    "        y_odd = x_even*sin_expend+x_odd*cos_expend\n",
    "        y = rearrange(torch.stack([y_even, y_odd], dim=-1), '... s d two -> ... s (d two)')\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e58048",
   "metadata": {},
   "source": [
    "# Q 3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b29a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim=dim, keepdim=True).values\n",
    "    x_subtract_max = x-x_max\n",
    "    x_subtract_max_exp = torch.exp(x_subtract_max)\n",
    "    x_subtract_max_exp_sum = torch.sum(x_subtract_max_exp, dim=dim, keepdim=True)\n",
    "    y = x_subtract_max_exp/x_subtract_max_exp_sum\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cff50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    mask: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    d_k = Q.shape[-1]\n",
    "    QK = einsum(\n",
    "        Q, K, \"... n d_k, ... m d_k -> ... n m\"\n",
    "    )\n",
    "    QK_scaled = QK/torch.tensor(d_k).sqrt()\n",
    "    if mask is not None:\n",
    "        M = torch.where(mask, torch.tensor(0.0), torch.tensor(float('-inf')))\n",
    "        QK_scaled += M\n",
    "    QK_scaled_softmax = softmax(QK_scaled, Q.dim()-1)\n",
    "    y = einsum(\n",
    "        QK_scaled_softmax, V, \"... n m, ... m d_v -> ... n d_v\"\n",
    "    )\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f6f3b",
   "metadata": {},
   "source": [
    "# 3.5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b152a9e",
   "metadata": {},
   "source": [
    "### Step1: 让我们从没有head 和 batch的简单情况开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d39f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 1\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 1\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9a254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.3022,  3.5587,  0.0493,  2.1251],\n",
       "         [ 6.2906,  3.5453,  0.0229,  2.2065],\n",
       "         [ 6.2706,  3.5209, -0.0067,  2.2971]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q,K,V)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d2c29",
   "metadata": {},
   "source": [
    "### Step2: 加入batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ea4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 1\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a1cfee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.4333,  -2.2128,   1.1368,  -2.7840],\n",
       "         [ -4.1671,  -0.1571,   1.0785, -19.5492],\n",
       "         [ -3.1011,  -2.1886,   1.3718,  -5.7030]],\n",
       "\n",
       "        [[  3.7037,   2.6312,  -1.1190,  13.7237],\n",
       "         [  0.0890,  -2.3049,  -0.1087,  -2.6351],\n",
       "         [  0.1030,  -2.2935,  -0.1119,  -2.5340]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q,K,V)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b8ea9",
   "metadata": {},
   "source": [
    "### Step3: 加入head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeb8027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fc48e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9310,  6.0378, -0.0974, -0.8215],\n",
       "         [10.2525,  5.0840, -8.1525, -7.2442],\n",
       "         [-3.5506,  2.4867,  1.0241,  1.9592]],\n",
       "\n",
       "        [[-3.3017, -6.0040,  7.7340,  6.5704],\n",
       "         [ 3.6543, 11.7540,  0.8076, -8.0421],\n",
       "         [ 4.2321, -2.0617, -0.4722, -4.8180]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head,K_head,V_head)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7be6f235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4]), torch.Size([2, 3, 4]), torch.Size([2, 3, 16])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[attention.shape, in_features.shape,head.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40641e7b",
   "metadata": {},
   "source": [
    "### Step4: 加入mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eebded8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d75906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.6790,  -2.4998,  -8.9383,   4.1447],\n",
       "         [ 10.3599,   5.2157,  -8.1922,  -7.3385],\n",
       "         [ -3.5506,   2.4867,   1.0241,   1.9592]],\n",
       "\n",
       "        [[-12.1002,  -3.7207,   2.2474,   7.5827],\n",
       "         [ -8.2128,   6.3416,  -2.3230,  -5.1408],\n",
       "         [  4.2321,  -2.0617,  -0.4722,  -4.8180]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "mask_boardcasted = mask.expand(expend_shape)\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head,K_head,V_head,mask_boardcasted)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9a924",
   "metadata": {},
   "source": [
    "### Step5: 加入RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "405fca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfa1d0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4]), torch.Size([2, 2, 3, 6])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[in_features.shape,Q_head.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f8918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.6790,  -2.4998,  -8.9383,   4.1447],\n",
       "         [  7.7346,   1.0026,  -6.7823,  -3.6310],\n",
       "         [ -7.4218,  -2.0985,   2.3776,   5.1274]],\n",
       "\n",
       "        [[-12.1002,  -3.7207,   2.2474,   7.5827],\n",
       "         [ -8.0947,   6.5038,  -2.3712,  -5.3683],\n",
       "         [  9.2953,   0.3954,   7.2983,  -6.3640]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = 10000\n",
    "max_seq_len = 10\n",
    "rope = RotaryPositionalEmbedding(theta, d_k, max_seq_len)\n",
    "position = torch.arange(seq_len)\n",
    "\n",
    "\n",
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "mask_boardcasted = mask.expand(expend_shape)\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "position_expend_shape = (Q_head.shape[:-1])\n",
    "position = position.expand(position_expend_shape)\n",
    "Q_head_rope = rope(Q_head, position)\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K_head_rope = rope(K_head, position)\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head_rope,K_head_rope,V_head,mask_boardcasted)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc582161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, \n",
    "                theta: float|None=None, max_seq_len: int|None=None,\n",
    "                device: torch.device | None = None,dtype: torch.dtype | None = None,):\n",
    "        super().__init__()\n",
    "        ## mplement causal multi-head self-attention\n",
    "        self.d_model = d_model ## final dimension of the input\n",
    "        self.num_heads = num_heads ## number of heads\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.theta = theta\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        assert d_model%num_heads == 0, \"d_model/num_heads need to be int\"\n",
    "        self.d_k = d_model//num_heads\n",
    "        self.d_v = d_model//num_heads\n",
    "        \n",
    "        # self.W_q = Linear(d_model,self.d_k * num_heads,device,dtype)\n",
    "        # self.W_k = Linear(d_model,self.d_k * num_heads,device,dtype)\n",
    "        # self.W_v = Linear(d_model,self.d_v * num_heads,device,dtype)\n",
    "        # self.W_o = Linear(self.d_k * num_heads,d_model,device,dtype)\n",
    "        \n",
    "        self.W_q = nn.Parameter(torch.randn(self.d_k * num_heads, d_model))\n",
    "        self.W_k = nn.Parameter(torch.randn(self.d_k * num_heads, d_model))\n",
    "        self.W_v = nn.Parameter(torch.randn(self.d_v * num_heads, d_model))\n",
    "        self.W_o = nn.Parameter(torch.randn(d_model, self.d_v * num_heads))\n",
    "        \n",
    "        self.rope = None\n",
    "        if (theta is not None) and (max_seq_len is not None):\n",
    "            self.rope = RotaryPositionalEmbedding(theta, self.d_k, max_seq_len,device,dtype)\n",
    "    \n",
    "    def forward(self, in_features: torch.Tensor, token_positions: torch.Tensor|None=None) -> torch.Tensor:\n",
    "        seq_len = in_features.shape[-2]\n",
    "        mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "        # Q = self.W_q(in_features)\n",
    "        Q = einsum(\n",
    "            self.W_q, in_features,\n",
    "            \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    "        )\n",
    "        Q_head = rearrange(\n",
    "            Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = self.num_heads\n",
    "        )\n",
    "        # K = self.W_k(in_features)\n",
    "        K = einsum(\n",
    "            self.W_k, in_features,\n",
    "            \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    "        )\n",
    "        K_head = rearrange(\n",
    "            K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = self.num_heads\n",
    "        )\n",
    "        if (self.rope is not None) and (token_positions is not None):\n",
    "            position = repeat(\n",
    "                token_positions, \" ... seq_len -> ... n seq_len\", n = self.num_heads\n",
    "            )\n",
    "            Q_head = self.rope(Q_head, position)\n",
    "            K_head = self.rope(K_head, position)\n",
    "        # V = self.W_v(in_features)\n",
    "        V = einsum(\n",
    "            self.W_v, in_features,\n",
    "            \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    "        )\n",
    "        V_head = rearrange(\n",
    "            V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = self.num_heads\n",
    "        )\n",
    "        expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "        mask_boardcasted = mask.expand(expend_shape)\n",
    "        head = scaled_dot_product_attention(Q_head,K_head,V_head,mask_boardcasted)\n",
    "        head = rearrange(\n",
    "            head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    "        )\n",
    "        # attention = self.W_o(head)\n",
    "        attention = einsum(\n",
    "            head, self.W_o,\n",
    "            \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    "        )\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf921a3",
   "metadata": {},
   "source": [
    "# Q 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8003617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "num_heads = 1\n",
    "d_ff = 6\n",
    "device = None\n",
    "dtype = None\n",
    "\n",
    "\n",
    "batch = 1\n",
    "seq_len = 3\n",
    "\n",
    "# 输入张量\n",
    "x = torch.randn(batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "832d665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_norm1 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "rms_norm2 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "multi_head_attention = MultiheadSelfAttention(d_model, num_heads , device = device, dtype = dtype)\n",
    "ffn = SwiGLU(d_model, d_ff ,device = device, dtype = dtype)\n",
    "x += multi_head_attention(rms_norm1(x))\n",
    "x += ffn(rms_norm2(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b3c9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, \n",
    "                theta: float|None=None, max_seq_len: int|None=None,\n",
    "                device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.theta = theta\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.rms_norm1 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "        self.rms_norm2 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "        self.mha = MultiheadSelfAttention(d_model, num_heads, theta=theta,max_seq_len=max_seq_len, device = device, dtype = dtype)\n",
    "        self.ffn = SwiGLU(d_model=d_model, d_ff=d_ff ,device = device, dtype = dtype)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x += self.mha(self.rms_norm1(x))\n",
    "        x += self.ffn(self.rms_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea015132",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerBlock(d_model,num_heads,d_ff,theta,max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53f9e9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.3757, -0.5687,  0.0147,  0.1320],\n",
       "        [ 1.6708, -0.2414,  1.5715, -2.7058],\n",
       "        [ 1.3684, -0.2810,  0.6613,  0.6166],\n",
       "        [-2.2904,  1.2070,  0.0746,  0.7860]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mha.W_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16b8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
