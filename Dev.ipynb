{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import regex as re\n",
    "# from multiprocessing import Pool\n",
    "# from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "# from memory_profiler import profile\n",
    "# import time, tracemalloc\n",
    "# from dataclasses import dataclass\n",
    "# from typing import BinaryIO, Iterable, Iterator\n",
    "# import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, einsum, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda09a5",
   "metadata": {},
   "source": [
    "## Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b3d4",
   "metadata": {},
   "source": [
    "### Q 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        ## Construct a linear transformation module. This function should accept the following parameters:\n",
    "        self.in_features = in_features ## final dimension of the input\n",
    "        self.out_features = out_features ## final dimension of the output\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.empty(out_features, in_features, device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        std = torch.sqrt(torch.tensor(2.0/(in_features+out_features)))\n",
    "        nn.init.trunc_normal_(self.weights, mean=0.0, std=std.item(), a=-3*std.item(), b=3*std.item())\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Apply the linear transformation to the input\n",
    "        output = einsum(\n",
    "            x, self.weights,\n",
    "            \"... in_dim, out_dim in_dim -> ... out_dim\"\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b05a99",
   "metadata": {},
   "source": [
    "### Q 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct an embedding module\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings ## Size of the vocabulary\n",
    "        self.embedding_dim = embedding_dim ## Dimension of the embedding vectors\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "        \n",
    "        self.weights = nn.Parameter(\n",
    "            torch.empty(num_embeddings, embedding_dim, device=self.device, dtype=self.dtype)\n",
    "        )\n",
    "        std = 1.0\n",
    "        nn.init.trunc_normal_(self.weights, mean=0.0, std=std,a=-3,b=3)\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        ## Lookup the embedding vectors for the given token IDs.\n",
    "        return self.weights[token_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06587e",
   "metadata": {},
   "source": [
    "### Q 3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31176275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct the RMSNorm module.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        self.eps = eps ## Epsilon value for numerical stability\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(d_model, device=self.device, dtype=self.dtype))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Process an input tensor of shape\n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x_squaremean = reduce(\n",
    "            x**2, \"... d_model -> ... 1\", 'mean'\n",
    "        )\n",
    "        x_RMS = (x_squaremean+self.eps).sqrt()\n",
    "        result = x / x_RMS * self.weights\n",
    "        return result.to(in_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6984d27",
   "metadata": {},
   "source": [
    "### Q 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        if d_ff is None:\n",
    "            q = round(d_model*8/3/64)\n",
    "            self.d_ff = q*64\n",
    "        else:\n",
    "            self.d_ff = d_ff\n",
    "        \n",
    "        self.w1_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model, device=self.device, dtype=self.dtype))\n",
    "        self.w2_weight = nn.Parameter(torch.randn(self.d_model, self.d_ff, device=self.device, dtype=self.dtype))\n",
    "        self.w3_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model, device=self.device, dtype=self.dtype))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w1x = einsum(\n",
    "            self.w1_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        w3x = einsum(\n",
    "            self.w3_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        SiLUw1x = w1x*torch.sigmoid(w1x)\n",
    "        part2 = SiLUw1x * w3x\n",
    "        result = einsum(\n",
    "            self.w2_weight, part2,\n",
    "            \"d_model d_ff, ... d_ff -> ... d_model\"\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfda749",
   "metadata": {},
   "source": [
    "### Q 3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21088daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, rope_theta: float, d_k: int, max_seq_len: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct the RoPE module and create buffers if needed.\n",
    "        super().__init__()\n",
    "        assert d_k % 2 == 0, \"RoPE requires even head dimension (pairs of features)\"\n",
    "        self.rope_theta = rope_theta ## $\\\\Theta$ value for the RoPE\n",
    "        self.d_k = d_k ## dimension of query and key vectors\n",
    "        self.max_seq_len = max_seq_len ## Maximum sequence length that will be inputted\n",
    "        self.device = device ## Device to store the buffer on\n",
    "\n",
    "        dim_index = torch.arange(self.d_k // 2, device=self.device, dtype=torch.float32)\n",
    "        position_index = torch.arange(self.max_seq_len, device=self.device, dtype=torch.float32)\n",
    "        theta_inv_index = self.rope_theta**(-2*dim_index/d_k)\n",
    "        theta_ik = einsum(\n",
    "            position_index, theta_inv_index,\n",
    "            \"s, d -> s d\"\n",
    "        )\n",
    "        sin = torch.sin(theta_ik)\n",
    "        cos = torch.cos(theta_ik)\n",
    "        \n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        assert x.shape[-1] == self.d_k, \"The last dim of input should be equal to dim of embedding.\"\n",
    "        assert x.shape[-2] == token_positions.shape[-1], \"token_positions length must match sequence length\"\n",
    "        sin_expend = self.sin[token_positions]\n",
    "        cos_expend = self.cos[token_positions]\n",
    "\n",
    "        x_even = x[...,::2]\n",
    "        x_odd = x[...,1::2]\n",
    "\n",
    "        y_even = x_even*cos_expend-x_odd*sin_expend\n",
    "        y_odd = x_even*sin_expend+x_odd*cos_expend\n",
    "        y = rearrange(torch.stack([y_even, y_odd], dim=-1), '... s d two -> ... s (d two)')\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e58048",
   "metadata": {},
   "source": [
    "### Q 3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b29a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim=dim, keepdim=True).values\n",
    "    x_subtract_max = x-x_max\n",
    "    x_subtract_max_exp = torch.exp(x_subtract_max)\n",
    "    x_subtract_max_exp_sum = torch.sum(x_subtract_max_exp, dim=dim, keepdim=True)\n",
    "    y = x_subtract_max_exp/x_subtract_max_exp_sum\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cff50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    mask: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    d_k = Q.shape[-1]\n",
    "    QK = einsum(\n",
    "        Q, K, \"... n d_k, ... m d_k -> ... n m\"\n",
    "    )\n",
    "    QK_scaled = QK/torch.tensor(d_k).sqrt()\n",
    "    if mask is not None:\n",
    "        M = torch.where(mask, torch.tensor(0.0), torch.tensor(float('-inf')))\n",
    "        QK_scaled += M\n",
    "    QK_scaled_softmax = softmax(QK_scaled, Q.dim()-1)\n",
    "    y = einsum(\n",
    "        QK_scaled_softmax, V, \"... n m, ... m d_v -> ... n d_v\"\n",
    "    )\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f6f3b",
   "metadata": {},
   "source": [
    "### 3.5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b152a9e",
   "metadata": {},
   "source": [
    "#### Step1: 让我们从没有head 和 batch的简单情况开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d39f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 1\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 1\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e9a254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.3022,  3.5587,  0.0493,  2.1251],\n",
       "         [ 6.2906,  3.5453,  0.0229,  2.2065],\n",
       "         [ 6.2706,  3.5209, -0.0067,  2.2971]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q,K,V)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d2c29",
   "metadata": {},
   "source": [
    "#### Step2: 加入batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ea4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 1\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a1cfee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.4333,  -2.2128,   1.1368,  -2.7840],\n",
       "         [ -4.1671,  -0.1571,   1.0785, -19.5492],\n",
       "         [ -3.1011,  -2.1886,   1.3718,  -5.7030]],\n",
       "\n",
       "        [[  3.7037,   2.6312,  -1.1190,  13.7237],\n",
       "         [  0.0890,  -2.3049,  -0.1087,  -2.6351],\n",
       "         [  0.1030,  -2.2935,  -0.1119,  -2.5340]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"d_k d_in, ... d_in -> ... d_k\"\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"d_v d_in, ... d_in -> ... d_v\"\n",
    ")\n",
    "head = scaled_dot_product_attention(Q,K,V)\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b8ea9",
   "metadata": {},
   "source": [
    "#### Step3: 加入head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb8027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc48e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9310,  6.0378, -0.0974, -0.8215],\n",
       "         [10.2525,  5.0840, -8.1525, -7.2442],\n",
       "         [-3.5506,  2.4867,  1.0241,  1.9592]],\n",
       "\n",
       "        [[-3.3017, -6.0040,  7.7340,  6.5704],\n",
       "         [ 3.6543, 11.7540,  0.8076, -8.0421],\n",
       "         [ 4.2321, -2.0617, -0.4722, -4.8180]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = torch.tensor(q_proj_weight.shape[-2])\n",
    "\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head,K_head,V_head)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be6f235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4]), torch.Size([2, 3, 4]), torch.Size([2, 3, 16])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[attention.shape, in_features.shape,head.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40641e7b",
   "metadata": {},
   "source": [
    "#### Step4: 加入mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eebded8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d75906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.6790,  -2.4998,  -8.9383,   4.1447],\n",
       "         [ 10.3599,   5.2157,  -8.1922,  -7.3385],\n",
       "         [ -3.5506,   2.4867,   1.0241,   1.9592]],\n",
       "\n",
       "        [[-12.1002,  -3.7207,   2.2474,   7.5827],\n",
       "         [ -8.2128,   6.3416,  -2.3230,  -5.1408],\n",
       "         [  4.2321,  -2.0617,  -0.4722,  -4.8180]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "mask_boardcasted = mask.expand(expend_shape)\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head,K_head,V_head,mask_boardcasted)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9a924",
   "metadata": {},
   "source": [
    "#### Step5: 加入RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "405fca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 8\n",
    "num_heads = 2\n",
    "torch.manual_seed(4)\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfa1d0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4]), torch.Size([2, 2, 3, 6])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[in_features.shape,Q_head.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b6f8918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.6790,  -2.4998,  -8.9383,   4.1447],\n",
       "         [  7.7346,   1.0026,  -6.7823,  -3.6310],\n",
       "         [ -7.4218,  -2.0985,   2.3776,   5.1274]],\n",
       "\n",
       "        [[-12.1002,  -3.7207,   2.2474,   7.5827],\n",
       "         [ -8.0947,   6.5038,  -2.3712,  -5.3683],\n",
       "         [  9.2953,   0.3954,   7.2983,  -6.3640]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rope_theta = 10000\n",
    "max_seq_len = 10\n",
    "rope = RotaryPositionalEmbedding(rope_theta, d_k, max_seq_len)\n",
    "position = torch.arange(seq_len)\n",
    "\n",
    "\n",
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "mask_boardcasted = mask.expand(expend_shape)\n",
    "Q = einsum(\n",
    "    q_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "Q_head = rearrange(\n",
    "    Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "position_expend_shape = (Q_head.shape[:-1])\n",
    "position = position.expand(position_expend_shape)\n",
    "Q_head_rope = rope(Q_head, position)\n",
    "K = einsum(\n",
    "    k_proj_weight, in_features,\n",
    "    \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    ")\n",
    "K_head = rearrange(\n",
    "    K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = num_heads\n",
    ")\n",
    "K_head_rope = rope(K_head, position)\n",
    "V = einsum(\n",
    "    v_proj_weight, in_features,\n",
    "    \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    ")\n",
    "V_head = rearrange(\n",
    "    V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = num_heads\n",
    ")\n",
    "head = scaled_dot_product_attention(Q_head_rope,K_head_rope,V_head,mask_boardcasted)\n",
    "head = rearrange(\n",
    "    head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    ")\n",
    "attention = einsum(\n",
    "    head, o_proj_weight,\n",
    "    \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    ")\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc582161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, \n",
    "                rope_theta: float|None=None, max_seq_len: int|None=None,\n",
    "                device: torch.device | None = None,dtype: torch.dtype | None = None,):\n",
    "        super().__init__()\n",
    "        ## mplement causal multi-head self-attention\n",
    "        self.d_model = d_model ## final dimension of the input\n",
    "        self.num_heads = num_heads ## number of heads\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.rope_theta = rope_theta\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        assert d_model%num_heads == 0, \"d_model/num_heads need to be int\"\n",
    "        self.d_k = d_model//num_heads\n",
    "        self.d_v = d_model//num_heads\n",
    "        \n",
    "        # self.W_q = Linear(d_model,self.d_k * num_heads,device,dtype)\n",
    "        # self.W_k = Linear(d_model,self.d_k * num_heads,device,dtype)\n",
    "        # self.W_v = Linear(d_model,self.d_v * num_heads,device,dtype)\n",
    "        # self.W_o = Linear(self.d_k * num_heads,d_model,device,dtype)\n",
    "        \n",
    "        self.W_q = nn.Parameter(torch.randn(self.d_k * num_heads, d_model))\n",
    "        self.W_k = nn.Parameter(torch.randn(self.d_k * num_heads, d_model))\n",
    "        self.W_v = nn.Parameter(torch.randn(self.d_v * num_heads, d_model))\n",
    "        self.W_o = nn.Parameter(torch.randn(d_model, self.d_v * num_heads))\n",
    "        \n",
    "        self.rope = None\n",
    "        if (rope_theta is not None) and (max_seq_len is not None):\n",
    "            self.rope = RotaryPositionalEmbedding(rope_theta, self.d_k, max_seq_len,device,dtype)\n",
    "    \n",
    "    def forward(self, in_features: torch.Tensor, token_positions: torch.Tensor|None=None) -> torch.Tensor:\n",
    "        seq_len = in_features.shape[-2]\n",
    "        mask = torch.tril(torch.ones(seq_len,seq_len,dtype=torch.bool))\n",
    "        # Q = self.W_q(in_features)\n",
    "        Q = einsum(\n",
    "            self.W_q, in_features,\n",
    "            \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    "        )\n",
    "        Q_head = rearrange(\n",
    "            Q, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = self.num_heads\n",
    "        )\n",
    "        # K = self.W_k(in_features)\n",
    "        K = einsum(\n",
    "            self.W_k, in_features,\n",
    "            \"nd_k d_in, ... d_in -> ... nd_k\"\n",
    "        )\n",
    "        K_head = rearrange(\n",
    "            K, \"... seq_len (n d_k) -> ... n seq_len d_k\", n = self.num_heads\n",
    "        )\n",
    "        if (self.rope is not None) and (token_positions is not None):\n",
    "            position = repeat(\n",
    "                token_positions, \" ... seq_len -> ... n seq_len\", n = self.num_heads\n",
    "            )\n",
    "            Q_head = self.rope(Q_head, position)\n",
    "            K_head = self.rope(K_head, position)\n",
    "        # V = self.W_v(in_features)\n",
    "        V = einsum(\n",
    "            self.W_v, in_features,\n",
    "            \"nd_v d_in, ... d_in -> ... nd_v\"\n",
    "        )\n",
    "        V_head = rearrange(\n",
    "            V, \"... seq_len (n d_v) -> ... n seq_len d_v\", n = self.num_heads\n",
    "        )\n",
    "        expend_shape = (*Q_head.shape[:-1], seq_len)\n",
    "        mask_boardcasted = mask.expand(expend_shape)\n",
    "        head = scaled_dot_product_attention(Q_head,K_head,V_head,mask_boardcasted)\n",
    "        head = rearrange(\n",
    "            head, \"... n seq_len d_v -> ... seq_len (n d_v)\"\n",
    "        )\n",
    "        # attention = self.W_o(head)\n",
    "        attention = einsum(\n",
    "            head, self.W_o,\n",
    "            \"... seq_len d_v,  d_in d_v -> ... seq_len d_in\"\n",
    "        )\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf921a3",
   "metadata": {},
   "source": [
    "### Q 3.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8003617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "num_heads = 1\n",
    "d_ff = 6\n",
    "device = None\n",
    "dtype = None\n",
    "\n",
    "\n",
    "batch = 1\n",
    "seq_len = 3\n",
    "\n",
    "# 输入张量\n",
    "x = torch.randn(batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "832d665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_norm1 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "rms_norm2 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "multi_head_attention = MultiheadSelfAttention(d_model, num_heads , device = device, dtype = dtype)\n",
    "ffn = SwiGLU(d_model, d_ff ,device = device, dtype = dtype)\n",
    "x += multi_head_attention(rms_norm1(x))\n",
    "x += ffn(rms_norm2(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b3c9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, \n",
    "                rope_theta: float|None=None, max_seq_len: int|None=None,\n",
    "                device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.rope_theta = rope_theta\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.rms_norm1 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "        self.rms_norm2 = RMSNorm(d_model, device = device, dtype = dtype)\n",
    "        self.mha = MultiheadSelfAttention(\n",
    "            d_model, \n",
    "            num_heads, \n",
    "            rope_theta=rope_theta,\n",
    "            max_seq_len=max_seq_len, \n",
    "            device = device, \n",
    "            dtype = dtype\n",
    "        )\n",
    "        self.ffn = SwiGLU(d_model=d_model, d_ff=d_ff ,device = device, dtype = dtype)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x += self.mha(self.rms_norm1(x))\n",
    "        x += self.ffn(self.rms_norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9e9d1",
   "metadata": {},
   "source": [
    "### Q 3.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfc9c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "from support.bpe_tokenize import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "304bffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'owt_train'\n",
    "vocab_size = 32000\n",
    "vocab_path = f'./data/outputs/{data_name}_{vocab_size}_vocab.pkl'\n",
    "merges_path = f'./data/outputs/{data_name}_{vocab_size}_merges.pkl'\n",
    "with open(vocab_path, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "with open(merges_path, \"rb\") as f:\n",
    "    merges = pickle.load(f)\n",
    "    \n",
    "bpe_tokenize = Tokenizer(vocab, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96ac10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_put = r'Hi. I am Yifan Li.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c16b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000\n",
    "context_length = 10\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec0c3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4\n",
    "num_heads = 2\n",
    "rope_theta = 10000\n",
    "d_ff = 8\n",
    "\n",
    "num_embeddings = vocab_size\n",
    "d_model=embedding_dim\n",
    "max_seq_len=context_length\n",
    "\n",
    "in_features = d_model\n",
    "out_features = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cde63cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(\n",
    "    num_embeddings = num_embeddings,\n",
    "    embedding_dim = embedding_dim\n",
    ")\n",
    "transformer = TransformerBlock(\n",
    "    d_model=d_model,\n",
    "    num_heads = num_heads,\n",
    "    d_ff=d_ff,\n",
    "    rope_theta = rope_theta,\n",
    "    max_seq_len=max_seq_len\n",
    ")\n",
    "rms_norm = RMSNorm(d_model)\n",
    "linear = Linear(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afcc1034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 32000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = bpe_tokenize.encode(in_put)\n",
    "x = embedding(token_ids)\n",
    "x = transformer(x)\n",
    "x = rms_norm(x)\n",
    "x = linear(x)\n",
    "x = softmax(x, 1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ebe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, context_length: int, num_layers: int, \n",
    "        d_model: int, num_heads: int, rope_theta: float,\n",
    "        device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.rope_theta = rope_theta\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        \n",
    "        self.embedding = Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = d_model\n",
    "        )\n",
    "        self.transformer = TransformerBlock(\n",
    "            d_model=d_model,\n",
    "            num_heads = num_heads,\n",
    "            d_ff=d_model, ## \n",
    "            rope_theta = rope_theta,\n",
    "            max_seq_len=context_length\n",
    "        )\n",
    "        self.rms_norm = RMSNorm(d_model)\n",
    "        self.linear = Linear(\n",
    "            in_features=d_model,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_indices: torch.Tensor) -> torch.Tensor:\n",
    "        x = embedding(in_indices)\n",
    "        x = transformer(x)\n",
    "        x = rms_norm(x)\n",
    "        x = linear(x)\n",
    "        x = softmax(x, 1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf16c8",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0632a70",
   "metadata": {},
   "source": [
    "### Q 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b85c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9549, -0.3947,  1.3685, -0.5727],\n",
       "        [ 1.7438, -1.0567, -0.6034,  1.8716]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 2\n",
    "vocab_size = 4\n",
    "\n",
    "oi = torch.randn(batch, vocab_size)\n",
    "x = torch.randint(low=0, high=vocab_size-1, size=(batch,))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1f78b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7622],\n",
       "        [-0.8596]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi_max = torch.max(oi, dim=-1, keepdim=True).values\n",
    "oi_subtract_max = oi-oi_max\n",
    "oi_subtract_max_exp = oi_subtract_max.exp()\n",
    "oi_subtract_max_exp_sum = torch.sum(oi_subtract_max_exp, dim=-1, keepdim=True)\n",
    "x_expanded = rearrange(x, 'n -> n 1')\n",
    "result = torch.gather(oi, 1, x_expanded)/oi_subtract_max_exp_sum.log()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81810d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(inputs: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    inputs_max = torch.max(inputs, dim=-1, keepdim=True).values\n",
    "    inputs_subtract_max = inputs - inputs_max\n",
    "    inputs_exp = inputs_subtract_max.exp()\n",
    "    inputs_exp_sum = torch.sum(inputs_exp, dim=-1, keepdim=True)\n",
    "\n",
    "    target_expanded = rearrange(target, 'n -> n 1')\n",
    "    target_logits = torch.gather(inputs, 1, target_expanded)\n",
    "    \n",
    "    loss = - (target_logits - torch.log(inputs_exp_sum))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ad10eac",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m inputs = torch.tensor([\u001b[32m2.0\u001b[39m,\u001b[32m1.0\u001b[39m,\u001b[32m0.1\u001b[39m])\n\u001b[32m      2\u001b[39m target = torch.tensor([\u001b[32m1\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(inputs, target)\u001b[39m\n\u001b[32m      5\u001b[39m inputs_subtract_max_exp_sum = torch.sum(inputs_subtract_max_exp, dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m target_expanded = rearrange(target, \u001b[33m'\u001b[39m\u001b[33mn -> n 1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_expanded\u001b[49m\u001b[43m)\u001b[49m/inputs_subtract_max_exp_sum.log()\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([2.0,1.0,0.1])\n",
    "target = torch.tensor([1])\n",
    "cross_entropy(inputs,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f4d9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_max = torch.max(inputs, dim=-1, keepdim=True).values\n",
    "inputs_subtract_max = inputs-inputs_max\n",
    "inputs_subtract_max_exp = inputs_subtract_max.exp()\n",
    "inputs_subtract_max_exp_sum = torch.sum(inputs_subtract_max_exp, dim=-1, keepdim=True)\n",
    "target_expanded = rearrange(target, 'n -> n 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "302d4495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 1.0000, 0.1000])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b928bff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_expanded\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.gather(inputs, 1, target_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef5aa1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a549d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
