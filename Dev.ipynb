{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import regex as re\n",
    "# from multiprocessing import Pool\n",
    "# from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "# from memory_profiler import profile\n",
    "# import time, tracemalloc\n",
    "# from dataclasses import dataclass\n",
    "# from typing import BinaryIO, Iterable, Iterator\n",
    "# import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, einsum, reduce, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b3d4",
   "metadata": {},
   "source": [
    "# Q 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "d12318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        ## Construct a linear transformation module. This function should accept the following parameters:\n",
    "        self.in_features = in_features ## final dimension of the input\n",
    "        self.out_features = out_features ## final dimension of the output\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        w = torch.empty(in_features, out_features)\n",
    "        std = torch.sqrt(torch.tensor(2.0/(in_features+out_features)))\n",
    "        self.weight = nn.Parameter(nn.init.trunc_normal_(w, mean=0.0, std=std.item(),a=-3*std.item(),b=3*std.item()))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Apply the linear transformation to the input\n",
    "        output = einsum(\n",
    "            self.weight, x,\n",
    "            \"in_dim out_dim, in_dim -> out_dim\"\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b05a99",
   "metadata": {},
   "source": [
    "# Q 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "fcb550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct an embedding module\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings ## Size of the vocabulary\n",
    "        self.embedding_dim = embedding_dim ## Dimension of the embedding vectors\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "        \n",
    "        w = torch.empty(num_embeddings, embedding_dim)\n",
    "        std = 1.0\n",
    "        self.weight = nn.Parameter(nn.init.trunc_normal_(w, mean=0.0, std=std,a=-3,b=3))\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        ## Lookup the embedding vectors for the given token IDs.\n",
    "        return self.weight[token_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06587e",
   "metadata": {},
   "source": [
    "# Q 3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "id": "31176275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        ## Construct the RMSNorm module.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        self.eps = eps ## Epsilon value for numerical stability\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Process an input tensor of shape\n",
    "        \n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x_squaremean = reduce(\n",
    "            x**2, \"... d_model -> ... 1\", 'mean'\n",
    "        )\n",
    "        x_RMS = (x_squaremean+self.eps).sqrt()\n",
    "        result = x / x_RMS * self.weights\n",
    "        return result.to(in_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6984d27",
   "metadata": {},
   "source": [
    "# Q 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "5cc353ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        if d_ff is None:\n",
    "            q = round(d_model*8/3/64)\n",
    "            self.d_ff = q*64\n",
    "        else:\n",
    "            self.d_ff = d_ff\n",
    "        \n",
    "        self.w1_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model))\n",
    "        self.w2_weight = nn.Parameter(torch.randn(self.d_model, self.d_ff))\n",
    "        self.w3_weight = nn.Parameter(torch.randn(self.d_ff, self.d_model))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w1x = einsum(\n",
    "            self.w1_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        w3x = einsum(\n",
    "            self.w3_weight, x,\n",
    "            \"d_ff d_model, ... d_model -> ... d_ff\"\n",
    "        )\n",
    "        SiLUw1x = w1x*torch.sigmoid(w1x)\n",
    "        part2 = SiLUw1x * w3x\n",
    "        result = einsum(\n",
    "            self.w2_weight, part2,\n",
    "            \"d_model d_ff, ... d_ff -> ... d_model\"\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfda749",
   "metadata": {},
   "source": [
    "# Q 3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "21088daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device: torch.device | None = None):\n",
    "        ## Construct the RoPE module and create buffers if needed.\n",
    "        super().__init__()\n",
    "        assert d_k % 2 == 0, \"RoPE requires even head dimension (pairs of features)\"\n",
    "        self.theta = theta ## $\\\\Theta$ value for the RoPE\n",
    "        self.d_k = d_k ## dimension of query and key vectors\n",
    "        self.max_seq_len = max_seq_len ## Maximum sequence length that will be inputted\n",
    "        self.device = device ## Device to store the buffer on\n",
    "\n",
    "        dim_index = torch.arange(self.d_k // 2, dtype=torch.float32)\n",
    "        position_index = torch.arange(self.max_seq_len, dtype=torch.float32)\n",
    "        theta_inv_index = self.theta**(-2*dim_index/d_k)\n",
    "        theta_ik = einsum(\n",
    "            position_index, theta_inv_index,\n",
    "            \"s, d -> s d\"\n",
    "        )\n",
    "        sin = torch.sin(theta_ik)\n",
    "        cos = torch.cos(theta_ik)\n",
    "        \n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        assert x.shape[-1] == self.d_k, \"The last dim of input should be equal to dim of embedding.\"\n",
    "        assert x.shape[-2] == token_positions.shape[-1], \"token_positions length must match sequence length\"\n",
    "        sin_expend = self.sin[token_positions]\n",
    "        cos_expend = self.cos[token_positions]\n",
    "\n",
    "        x_even = x[...,::2]\n",
    "        x_odd = x[...,1::2]\n",
    "\n",
    "        y_even = x_even*cos_expend-x_odd*sin_expend\n",
    "        y_odd = x_even*sin_expend+x_odd*cos_expend\n",
    "        y = rearrange(torch.stack([y_even, y_odd], dim=-1), '... s d two -> ... s (d two)')\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e58048",
   "metadata": {},
   "source": [
    "# Q 3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "6b29a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    x_max = torch.max(x, dim=dim, keepdim=True).values\n",
    "    x_subtract_max = x-x_max\n",
    "    x_subtract_max_exp = torch.exp(x_subtract_max)\n",
    "    x_subtract_max_exp_sum = torch.sum(x_subtract_max_exp, dim=dim, keepdim=True)\n",
    "    y = x_subtract_max_exp/x_subtract_max_exp_sum\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "0cff50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    mask: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    d_k = Q.shape[-1]\n",
    "    QK = einsum(\n",
    "        Q, K, \"... n d_k, ... m d_k -> ... n m\"\n",
    "    )\n",
    "    QK_scaled = QK/torch.tensor(d_k).sqrt()\n",
    "    if mask is not None:\n",
    "        M = torch.where(mask, torch.tensor(0.0), torch.tensor(float('-inf')))\n",
    "        QK_scaled += M\n",
    "    QK_scaled_softmax = softmax(QK_scaled, Q.dim()-1)\n",
    "    y = einsum(\n",
    "        QK_scaled_softmax, V, \"... n m, ... m d_v -> ... n d_v\"\n",
    "    )\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f6f3b",
   "metadata": {},
   "source": [
    "# 3.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "df9f73a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Float' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1023]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_multihead_self_attention\u001b[39m(\n\u001b[32m      2\u001b[39m     d_model: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m      3\u001b[39m     num_heads: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     q_proj_weight: \u001b[43mFloat\u001b[49m[Tensor, \u001b[33m\"\u001b[39m\u001b[33m d_k d_in\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m     k_proj_weight: Float[Tensor, \u001b[33m\"\u001b[39m\u001b[33m d_k d_in\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     v_proj_weight: Float[Tensor, \u001b[33m\"\u001b[39m\u001b[33m d_v d_in\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     o_proj_weight: Float[Tensor, \u001b[33m\"\u001b[39m\u001b[33m d_model d_v\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m     in_features: Float[Tensor, \u001b[33m\"\u001b[39m\u001b[33m ... sequence_length d_in\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      9\u001b[39m ) -> Float[Tensor, \u001b[33m\"\u001b[39m\u001b[33m ... sequence_length d_out\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Float' is not defined"
     ]
    }
   ],
   "source": [
    "def run_multihead_self_attention(\n",
    "    d_model: int,\n",
    "    num_heads: int,\n",
    "    q_proj_weight: Float[Tensor, \" d_k d_in\"],\n",
    "    k_proj_weight: Float[Tensor, \" d_k d_in\"],\n",
    "    v_proj_weight: Float[Tensor, \" d_v d_in\"],\n",
    "    o_proj_weight: Float[Tensor, \" d_model d_v\"],\n",
    "    in_features: Float[Tensor, \" ... sequence_length d_in\"],\n",
    ") -> Float[Tensor, \" ... sequence_length d_out\"]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "id": "908325ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置维度参数\n",
    "batch = 2\n",
    "seq_len = 3\n",
    "d_in = 4\n",
    "d_k = 6\n",
    "d_v = 5\n",
    "num_heads = 2\n",
    "\n",
    "# 输入张量\n",
    "in_features = torch.randn(batch, seq_len, d_in)\n",
    "in_features = torch.randn(seq_len, d_in)\n",
    "# Projection weights\n",
    "q_proj_weight = torch.randn(d_k * num_heads, d_in)   # (d_k * h, d_in)\n",
    "k_proj_weight = torch.randn(d_k * num_heads, d_in)\n",
    "v_proj_weight = torch.randn(d_v * num_heads, d_in)\n",
    "o_proj_weight = torch.randn(d_in, d_v * num_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multihead_self_attention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, max_seq_len: int, device: torch.device | None = None):\n",
    "        ## mplement causal multi-head self-attention\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "id": "521e0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "d_in = 16\n",
    "\n",
    "seq_len = in_features.shape[-2]\n",
    "\n",
    "\n",
    "def _generate_QKV(in_features, proj_weight,num_heads):\n",
    "    A = einsum(\n",
    "        in_features, proj_weight,\n",
    "        \"... d_in, nd_k d_in -> ... nd_k\"\n",
    "    )\n",
    "    A = rearrange(\n",
    "        A, \"... (n d_k) -> ... n d_k\", n = num_heads\n",
    "    )\n",
    "    return A\n",
    "\n",
    "theta = 10000\n",
    "max_seq_len = 10\n",
    "rope = RotaryPositionalEmbedding(theta, d_k, max_seq_len)\n",
    "\n",
    "def _rope_QKV(A, rope, seq_len):\n",
    "    A_rearrange = rearrange(\n",
    "        A, \"... seq n d_k -> ... n seq d_k\"\n",
    "    )\n",
    "    posi_shape = A_rearrange.shape[:-1]\n",
    "    posi_basic = torch.arange(seq_len)\n",
    "    token_positions = posi_basic.expand(posi_shape)\n",
    "    A_rope = rope(A_rearrange, token_positions)\n",
    "    A_final = rearrange(\n",
    "        A_rope, \"... n seq d_k -> ... seq n d_k\"\n",
    "    )\n",
    "    return A_final\n",
    "\n",
    "Qi = _generate_QKV(in_features, q_proj_weight, num_heads)\n",
    "Qi = _rope_QKV(Qi, rope, seq_len)\n",
    "Ki = _generate_QKV(in_features, k_proj_weight, num_heads)\n",
    "Ki = _rope_QKV(Ki, rope, seq_len)\n",
    "Vi = _generate_QKV(in_features, v_proj_weight, num_heads)\n",
    "\n",
    "mask_shape = Qi.shape[:-2] + (num_heads,num_heads)\n",
    "upper_tri = torch.triu(torch.ones((num_heads, num_heads), dtype=torch.bool), diagonal=0)\n",
    "mask = upper_tri.expand(mask_shape)\n",
    "MHi = scaled_dot_product_attention(Qi,Ki,Vi,mask)\n",
    "MH = rearrange(\n",
    "    MHi, \"... n d_v -> ... (n d_v)\", n = num_heads\n",
    ")\n",
    "y = einsum(\n",
    "    o_proj_weight, MH,\n",
    "    \"d_model nd_v, ... nd_v -> ... d_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Qi_t = rearrange(\n",
    "    Qi, \"... seq n d_k -> ... n seq d_k\"\n",
    ")\n",
    "posi_shape = Qi_t.shape[:-1]\n",
    "posi_basic = torch.arange(seq_len)\n",
    "token_positions = posi_basic.expand(posi_shape)\n",
    "Qi_t_rope = rope(Qi_t, token_positions)\n",
    "Qi_final = rearrange(\n",
    "    Qi_t_rope, \"... n seq d_k -> ... seq n d_k\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "id": "b82806b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 6])"
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qi_t_rope.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rope(in_query_or_key, token_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "61a4d27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3, 4]), torch.Size([3, 2, 6]), torch.Size([3, 12])]"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[in_features.shape,Qi.shape,Q.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041fc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac58829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device: torch.device | None = None):\n",
    "        ## Construct the RoPE module and create buffers if needed.\n",
    "        super().__init__()\n",
    "        assert d_k % 2 == 0, \"RoPE requires even head dimension (pairs of features)\"\n",
    "        self.theta = theta ## $\\\\Theta$ value for the RoPE\n",
    "        self.d_k = d_k ## dimension of query and key vectors\n",
    "        self.max_seq_len = max_seq_len ## Maximum sequence length that will be inputted\n",
    "        self.device = device ## Device to store the buffer on\n",
    "\n",
    "        dim_index = torch.arange(self.d_k // 2, dtype=torch.float32)\n",
    "        position_index = torch.arange(self.max_seq_len, dtype=torch.float32)\n",
    "        theta_inv_index = self.theta**(-2*dim_index/d_k)\n",
    "        theta_ik = einsum(\n",
    "            position_index, theta_inv_index,\n",
    "            \"s, d -> s d\"\n",
    "        )\n",
    "        sin = torch.sin(theta_ik)\n",
    "        cos = torch.cos(theta_ik)\n",
    "        \n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        assert x.shape[-1] == self.d_k, \"The last dim of input should be equal to dim of embedding.\"\n",
    "        assert x.shape[-2] == token_positions.shape[-1], \"token_positions length must match sequence length\"\n",
    "        sin_expend = self.sin[token_positions]\n",
    "        cos_expend = self.cos[token_positions]\n",
    "\n",
    "        x_even = x[...,::2]\n",
    "        x_odd = x[...,1::2]\n",
    "\n",
    "        y_even = x_even*cos_expend-x_odd*sin_expend\n",
    "        y_odd = x_even*sin_expend+x_odd*cos_expend\n",
    "        y = rearrange(torch.stack([y_even, y_odd], dim=-1), '... s d two -> ... s (d two)')\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
