{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from multiprocessing import Pool\n",
    "from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "from memory_profiler import profile\n",
    "import time, tracemalloc\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acb032",
   "metadata": {},
   "source": [
    "## Q1.1 Problem (train_bpe): BPE Tokenizer Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d6a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pair = tuple[int,int]\n",
    "Encoded_Token = tuple[int, ...]\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1b65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TokenMergePlan():\n",
    "    old_token: Encoded_Token\n",
    "    new_token: Encoded_Token\n",
    "    count: int\n",
    "    pair_positions: list[int]\n",
    "\n",
    "class PairFreqsDelta():\n",
    "    inc: defaultdict[Pair, int]\n",
    "    inc: defaultdict[Pair, int]\n",
    "    def __init__(self):\n",
    "        self.inc = defaultdict(int)\n",
    "        self.dec = defaultdict(int)\n",
    "\n",
    "class PairInhereitDelta():\n",
    "    add: defaultdict[Pair,Encoded_Token]\n",
    "    remove: defaultdict[Pair,Encoded_Token]\n",
    "    def __init__(self):\n",
    "        self.add = defaultdict(Encoded_Token)\n",
    "        self.remove = defaultdict(Encoded_Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4108944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(input_path: str) -> str:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_by_special_tokens(string: str, special_tokens: list[str]) -> list[str]:\n",
    "    pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "    return re.split(pattern,string)\n",
    "\n",
    "def count_tokens(string_list: list[str]) -> dict[str, int]:\n",
    "    counts = defaultdict(int)\n",
    "    for s in string_list:\n",
    "        tokens = re.finditer(PAT, s)\n",
    "        for m in tokens:\n",
    "            tok = m.group(0)\n",
    "            counts[tok] += 1\n",
    "    return counts\n",
    "\n",
    "def encode_and_count_tokens(counts: dict[str, int])-> dict[Encoded_Token, int]:\n",
    "    encoded_token_freqs = defaultdict(int)\n",
    "    for token, count in counts.items():\n",
    "        elements = tuple(token.encode(\"utf-8\"))\n",
    "        encoded_token_freqs[elements] += count\n",
    "    return encoded_token_freqs\n",
    "\n",
    "def build_initial_vocab(special_tokens: list[str]) ->  dict[int, bytes]:\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    for i, tok in enumerate(special_tokens, start=256):\n",
    "        vocab[i] = tok.encode(\"utf-8\")\n",
    "    return vocab\n",
    "\n",
    "def get_byte_pairs(encoded_token_freqs: dict[Encoded_Token, int]) -> dict[Pair, int]:\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for tok, count in encoded_token_freqs.items():\n",
    "        for i in range(len(tok)-1):\n",
    "            pair_freqs[(tok[i],tok[i+1])] += count\n",
    "    return pair_freqs\n",
    "\n",
    "def get_byte_pairs_inhereit(encoded_token_freqs: dict[Encoded_Token, int]) -> dict[Pair, set[Encoded_Token]]:\n",
    "    pair_inhereit = defaultdict(set)\n",
    "    for tok, count in encoded_token_freqs.items():\n",
    "        for i in range(len(tok)-1):\n",
    "            pair_inhereit[(tok[i],tok[i+1])].add(tok)\n",
    "    return pair_inhereit\n",
    "\n",
    "def select_merge_pair(pair_freqs: dict[Pair, int], vocab:dict[int, bytes]) -> Pair:\n",
    "    max_count = max(pair_freqs.values())\n",
    "    candidate_pairs = [key for key, value in pair_freqs.items() if value == max_count]\n",
    "    def sort_pair(pair):\n",
    "        index1, index2 = pair\n",
    "        return(vocab[index1], vocab[index2])\n",
    "    pair = max(candidate_pairs, key = sort_pair)\n",
    "    return pair\n",
    "\n",
    "def update_encoded_token(encoded_token: Encoded_Token, pair: Pair, new_index: int) -> Encoded_Token:\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(encoded_token):\n",
    "        if i < len(encoded_token) - 1 and (encoded_token[i], encoded_token[i + 1]) == pair:\n",
    "            result.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(encoded_token[i])\n",
    "            i += 1\n",
    "    return tuple(result)\n",
    "\n",
    "def find_subtuple_index(sequence: tuple, subseq: tuple) -> list[int]:\n",
    "    position = []\n",
    "    subseq_len = len(subseq)\n",
    "    for i in range(len(sequence)-subseq_len+1):\n",
    "        if sequence[i:i+subseq_len] == subseq:\n",
    "            position.append(i)\n",
    "    return position\n",
    "\n",
    "def remove_or_decrement_pair(pair_freqs: dict[Pair, int], pair: Pair, count: int) -> dict[Pair, int]:\n",
    "    updated_pair_freqs = pair_freqs.copy()\n",
    "    if updated_pair_freqs[pair] == count:\n",
    "        del updated_pair_freqs[pair]\n",
    "    else:\n",
    "        updated_pair_freqs[pair] -= count\n",
    "    return updated_pair_freqs\n",
    "\n",
    "def build_merge_plan(tok_need_update: set[Encoded_Token], encoded_token_freqs: dict[Encoded_Token, int], pair: Pair, new_index: int) -> list[TokenMergePlan]:\n",
    "    plan = []\n",
    "    for encoded_token in tok_need_update:\n",
    "        new_encoded_token = update_encoded_token(encoded_token, pair, new_index)\n",
    "        count = encoded_token_freqs[encoded_token]\n",
    "        pair_positions = find_subtuple_index(encoded_token,pair)\n",
    "        plan.append(TokenMergePlan(encoded_token,new_encoded_token,count,pair_positions))\n",
    "    return plan\n",
    "\n",
    "def update_encoded_token_freqs(plan: list[TokenMergePlan], encoded_token_freqs: dict[Encoded_Token, int]) ->  dict[Encoded_Token, int]:\n",
    "    new_encoded_token_freqs = encoded_token_freqs.copy()\n",
    "    for item in plan:\n",
    "        del new_encoded_token_freqs[item.old_token]\n",
    "        new_encoded_token_freqs[item.new_token] = item.count\n",
    "    return new_encoded_token_freqs\n",
    "\n",
    "def compute_freqs_and_inhereit_deltas(plan: list[TokenMergePlan], new_index:int) -> tuple[PairFreqsDelta, PairInhereitDelta]:\n",
    "    pair_freqs_d = PairFreqsDelta()\n",
    "    pair_inhereit_d = PairInhereitDelta()\n",
    "    for item in plan:\n",
    "        old_token = item.old_token\n",
    "        new_token = item.new_token\n",
    "        count = item.count\n",
    "\n",
    "        for pos in item.pair_positions:\n",
    "            if pos > 0:\n",
    "                pre_token = old_token[pos-1]\n",
    "                old_pair = (pre_token,old_token[pos])\n",
    "                new_pair = (pre_token, new_index)\n",
    "                pair_freqs_d.dec[old_pair] = count\n",
    "                pair_freqs_d.inc[new_pair] = count\n",
    "                pair_inhereit_d.add[new_pair] = new_token\n",
    "                pair_inhereit_d.remove[old_pair] = old_token\n",
    "            if pos < len(old_token)-2:\n",
    "                pos_token = old_token[pos+2]\n",
    "                old_pair = (old_token[pos+1],pos_token)\n",
    "                new_pair = (new_index, pos_token)\n",
    "                pair_freqs_d.dec[old_pair] = count\n",
    "                pair_freqs_d.inc[new_pair] = count\n",
    "                pair_inhereit_d.add[new_pair] = new_token\n",
    "                pair_inhereit_d.remove[old_pair] = old_token\n",
    "    return pair_freqs_d, pair_inhereit_d\n",
    "\n",
    "def exclude_pair_from_dict(d: dict, pair: Pair):\n",
    "    new_d = d.copy()\n",
    "    del new_d[pair]\n",
    "    return new_d\n",
    "\n",
    "def update_pair_freqs(pair_freqs: dict[Pair, int], pair_freqs_d: PairFreqsDelta):\n",
    "    new_pair_freqs = pair_freqs.copy()\n",
    "    for key, value in pair_freqs_d.dec.items():\n",
    "        new_pair_freqs = remove_or_decrement_pair(new_pair_freqs, key, value)\n",
    "    for key, value in pair_freqs_d.inc.items():\n",
    "        new_pair_freqs[key]+=value\n",
    "    return new_pair_freqs\n",
    "\n",
    "def update_pair_inhereit(pair_inhereit: dict[Pair, set[Encoded_Token]], pair_inhereit_d: PairInhereitDelta):\n",
    "    new_pair_inhereit = pair_inhereit.copy()\n",
    "    for key, value in pair_inhereit_d.remove.items():\n",
    "        new_pair_inhereit[key].discard(value)\n",
    "    for key, value in pair_inhereit_d.add.items():\n",
    "        new_pair_inhereit[key].add(value)\n",
    "    return new_pair_inhereit\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    vocab = build_initial_vocab(special_tokens)\n",
    "    vocab_len = len(vocab)\n",
    "    merges = []\n",
    "\n",
    "    string = read_text_file(input_path)\n",
    "    string_list = split_by_special_tokens(string,special_tokens)\n",
    "    token_count = count_tokens(string_list)\n",
    "    encoded_token_freqs = encode_and_count_tokens(token_count)\n",
    "    \n",
    "    pair_freqs = get_byte_pairs(encoded_token_freqs)\n",
    "    pair_inhereit = get_byte_pairs_inhereit(encoded_token_freqs)\n",
    "\n",
    "    while vocab_len < vocab_size:\n",
    "        pair = select_merge_pair(pair_freqs, vocab)\n",
    "        index1, index2 = pair\n",
    "        new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "        new_index = vocab_len\n",
    "        vocab[new_index] = new_token\n",
    "        merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "        tok_need_update = pair_inhereit[pair]\n",
    "        plan = build_merge_plan(tok_need_update, encoded_token_freqs, pair, new_index)\n",
    "        encoded_token_freqs = update_encoded_token_freqs(plan, encoded_token_freqs)\n",
    "        pair_freqs_d, pair_inhereit_d = compute_freqs_and_inhereit_deltas(plan, new_index)\n",
    "        pair_freqs = exclude_pair_from_dict(pair_freqs, pair)\n",
    "        pair_freqs = update_pair_freqs(pair_freqs, pair_freqs_d)\n",
    "        pair_inhereit = exclude_pair_from_dict(pair_inhereit, pair)\n",
    "        pair_inhereit = update_pair_inhereit(pair_inhereit, pair_inhereit_d)\n",
    "        vocab_len+=1\n",
    "        \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4f25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'\n",
    "\n",
    "epochs = 6\n",
    "vocab_size = 270\n",
    "string = \"hi. i'm yifan li. nice to meet you.<|endoftext|> this the what when here where\"\n",
    "# input_path = r'./data/TinyStoriesV2-GPT4-valid.txt'\n",
    "# string = read_text_file(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "308c9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 861.81 MiB, increment: 20.62 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7a435",
   "metadata": {},
   "source": [
    "## 个人优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0df35532",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 280\n",
    "string = \"hi. i'm yifan li. nice to meet you. <|endoftext|> this the what when here where. <|endoftext|> where is the car. <|endoftext|> when is dinner\"\n",
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a00f3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b785ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'h', b'e'),\n",
       " (b' ', b'w'),\n",
       " (b' ', b't'),\n",
       " (b'r', b'e'),\n",
       " (b'i', b's'),\n",
       " (b'w', b'he'),\n",
       " (b'whe', b'r'),\n",
       " (b'wher', b'e'),\n",
       " (b'w', b'h'),\n",
       " (b't', b'he'),\n",
       " (b'he', b'n'),\n",
       " (b'w', b'hen'),\n",
       " (b'h', b'i'),\n",
       " (b'e', b'r'),\n",
       " (b' ', b'y'),\n",
       " (b' ', b'where'),\n",
       " (b' ', b'when'),\n",
       " (b' ', b'the'),\n",
       " (b' ', b'is'),\n",
       " (b'o', b'u'),\n",
       " (b'n', b'n'),\n",
       " (b'n', b'i'),\n",
       " (b'ni', b'c')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c4a7d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'<|endoftext|>',\n",
       " 257: b'he',\n",
       " 258: b' w',\n",
       " 259: b' t',\n",
       " 260: b're',\n",
       " 261: b'is',\n",
       " 262: b'whe',\n",
       " 263: b'wher',\n",
       " 264: b'where',\n",
       " 265: b'wh',\n",
       " 266: b'the',\n",
       " 267: b'hen',\n",
       " 268: b'when',\n",
       " 269: b'hi',\n",
       " 270: b'er',\n",
       " 271: b' y',\n",
       " 272: b' where',\n",
       " 273: b' when',\n",
       " 274: b' the',\n",
       " 275: b' is',\n",
       " 276: b'ou',\n",
       " 277: b'nn',\n",
       " 278: b'ni',\n",
       " 279: b'nic'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e575e61",
   "metadata": {},
   "source": [
    "### Update merge logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e58262fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_initial_vocab(special_tokens)\n",
    "vocab_len = len(vocab)\n",
    "merges = []\n",
    "\n",
    "encoded_token_freqs = defaultdict(int)\n",
    "encoded_token_freqs[(1,2,3,4,2,3)] = 3\n",
    "encoded_token_freqs[(2,3,5)] = 2\n",
    "pair_freqs,pair_positions = get_byte_pairs_freqs_and_position(encoded_token_freqs)\n",
    "pair = select_merge_pair(pair_freqs, vocab)\n",
    "\n",
    "index1, index2 = pair\n",
    "new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "new_index = vocab_len\n",
    "vocab[new_index] = new_token\n",
    "vocab_len+=1\n",
    "merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "\n",
    "\n",
    "# pair_freqs = update_pair_freqs_after_merge(pair_freqs, positions, encoded_token_freqs, pair, new_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243a2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4, 2, 3)\n",
      "(2, 3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[defaultdict(int, {(1, 257): 3, (257, 4): 3, (4, 257): 3, (257, 5): 2}),\n",
       " {(1, 2, 3, 4, 2, 3): 3, (2, 3, 5): 2},\n",
       " defaultdict(set,\n",
       "             {(1, 2): {0},\n",
       "              (3, 4): {0},\n",
       "              (4, 2): {0},\n",
       "              (3, 5): {1},\n",
       "              (1, 257): {0},\n",
       "              (257, 4): {0},\n",
       "              (4, 257): {0},\n",
       "              (257, 5): {1}})]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_pair_positions = initiate_update(pair_positions,pair)\n",
    "updated_pair_freqs = initiate_update(pair_freqs,pair)\n",
    "\n",
    "encoded_token_map = {}\n",
    "positions = pair_positions[pair]\n",
    "for position in positions:\n",
    "    encoded_token, count = list(encoded_token_freqs.items())[position]\n",
    "    positions = find_subtuple_index(encoded_token,pair)\n",
    "    encoded_token_map[encoded_token] = update_encoded_token(encoded_token,positions, new_index)\n",
    "    for pos in positions:\n",
    "        if pos > 0:\n",
    "            pre_token = encoded_token[pos-1]\n",
    "            old_pair = (pre_token,encoded_token[pos])\n",
    "            new_pair = (pre_token, new_index)\n",
    "            updated_pair_freqs = remove_or_decrement_pair(updated_pair_freqs, old_pair, count)\n",
    "            updated_pair_freqs[new_pair]+=count\n",
    "            updated_pair_positions[new_pair].add(position)\n",
    "        if pos < len(encoded_token)-2:\n",
    "            pos_token = encoded_token[pos+2]\n",
    "            old_pair = (encoded_token[pos+1],pos_token)\n",
    "            new_pair = (new_index, pos_token)\n",
    "            updated_pair_freqs = remove_or_decrement_pair(updated_pair_freqs, old_pair, count)\n",
    "            updated_pair_freqs[new_pair]+=count\n",
    "            updated_pair_positions[new_pair].add(position)\n",
    "\n",
    "updated_encoded_token_freqs = update_encoded_token_freqs(encoded_token_freqs,encoded_token_map)\n",
    "[updated_pair_freqs,updated_encoded_token_freqs,updated_pair_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5daf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 3, 4, 2, 3): (1, 2, 3, 4, 2, 3), (2, 3, 5): (2, 3, 5)}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_encoded_token!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ef7e2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 0\n",
    "encoded_token, count = list(encoded_token_freqs.items())[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "783c9e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 257, 4, 257)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_encoded_token(encoded_token, pair, new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4700a25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 3, 4, 2, 3): (1, 2, 3, 4, 2, 3), (2, 3, 5): (2, 3, 5)}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_encoded_token(encoded_token, pair, new_index):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(encoded_token):\n",
    "        if i < len(encoded_token) - 1 and (encoded_token[i], encoded_token[i + 1]) == pair:\n",
    "            result.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(encoded_token[i])\n",
    "            i += 1\n",
    "    return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b1603452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4, 2, 3)\n",
      "(1, 2, 3, 4, 2, 3)\n",
      "(2, 3, 5)\n",
      "(2, 3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(1, 2, 3, 4, 2, 3): 3, (2, 3, 5): 2}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_encoded_token_freqs = {}\n",
    "for i, (key,value) in enumerate(encoded_token_freqs.items()):\n",
    "    if key in encoded_token_map:\n",
    "        print(key)\n",
    "        new_key = encoded_token_map[key]\n",
    "        print(new_key)\n",
    "        updated_encoded_token_freqs[new_key] = value\n",
    "    else:\n",
    "        updated_encoded_token_freqs[key] = value\n",
    "    # print(updated_encoded_token_freqs)\n",
    "updated_encoded_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6523b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_encoded_token_freqs(encoded_token_freqs: dict[list[int],int],encoded_token_map: dict[list[int], list[int]]) -> dict[list[int],int]:\n",
    "    updated_encoded_token_freqs = {}\n",
    "    for i, (key,value) in enumerate(encoded_token_freqs.items()):\n",
    "        if key in encoded_token_map:\n",
    "            new_key = encoded_token_map[key]\n",
    "            updated_encoded_token_freqs[new_key] = value\n",
    "        else:\n",
    "            updated_encoded_token_freqs[key] = value\n",
    "    return updated_encoded_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83457787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int, {(1, 257): 3, (257, 4): 3, (4, 257): 3, (257, 5): 2}),\n",
       " {(1, 2, 3, 4, 2, 3): 3, (2, 3, 5): 2},\n",
       " defaultdict(set,\n",
       "             {(1, 2): {0},\n",
       "              (3, 4): {0},\n",
       "              (4, 2): {0},\n",
       "              (3, 5): {1},\n",
       "              (1, 257): {0},\n",
       "              (257, 4): {0},\n",
       "              (4, 257): {0},\n",
       "              (257, 5): {1}})]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "updated_pair_positions, updated_pair_freqs, updated_encoded_token_freqs = update_all_structures(pair_positions, pair_freqs, encoded_token_freqs, pair, new_index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962e4e6",
   "metadata": {},
   "source": [
    "**updated_encoded_token_freqs is not correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "82cdc1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 257, 4, 257): 3, (257, 5): 2}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "295fa252",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 0\n",
    "encoded_token, count = list(encoded_token_freqs.items())[position]\n",
    "pair_positions = find_subtuple_index(encoded_token,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7980249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 257, 4, 257)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ae472dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_to_keys(A: dict, positions: list[int], F):\n",
    "    keys = list(A.keys())  # 保持 key 的顺序\n",
    "    for pos in positions:\n",
    "        old_key = keys[pos]\n",
    "        value = A.pop(old_key)         # 先取出旧值并删除旧 key\n",
    "        new_key = F(old_key)           # 使用函数 F 得到新 key\n",
    "        A[new_key] = value   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "08e48d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('b', 'c'): 2, ('A', 'B'): 1, ('C', 'D'): 3}\n"
     ]
    }
   ],
   "source": [
    "A = {('a', 'b'): 1, ('b', 'c'): 2, ('c', 'd'): 3}\n",
    "positions = [0, 2]\n",
    "\n",
    "# 假设你想把 key 的每个字符都变成大写\n",
    "def F(key):\n",
    "    return tuple(k.upper() for k in key)\n",
    "\n",
    "apply_function_to_keys(A, positions, F)\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76d05a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3, 4, 2, 3), 3]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_token, count = list(encoded_token_freqs.items())[position]\n",
    "[encoded_token,count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb687c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_positions = find_subtuple_index(encoded_token,pair)\n",
    "pair_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d2c62cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 8, (3, 4): 3, (4, 2): 3, (3, 5): 2})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004dace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_pair_freqs  = pair_freqs.copy()\n",
    "updated_pair_freqs[pair] -= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30091f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 5, (3, 4): 3, (3, 5): 2, (4, 257): 3})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_pair_freqs = pair_freqs.copy()\n",
    "for pair_position in pair_positions:\n",
    "    updated_pair_freqs = update_pair_freqs(pair_freqs, encoded_token, pair_position, count)\n",
    "updated_pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f65581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 8, (3, 4): 3, (4, 2): 3, (3, 5): 2})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_pair_freqs(pair_freqs: dict[tuple[int, int], int], encoded_token: tuple[int], pair_position: int, count: int) -> dict[tuple[int, int], int]:\n",
    "    updated_pair_freqs  = pair_freqs.copy()\n",
    "    updated_pair_freqs[pair] -= count\n",
    "    if pair_position > 0:\n",
    "        pre_token = encoded_token[pair_position-1]\n",
    "        pre_pair = (pre_token,encoded_token[pair_position])\n",
    "        new_pair = (pre_token, new_index)\n",
    "        if updated_pair_freqs[pre_pair] == count:\n",
    "            del updated_pair_freqs[pre_pair]\n",
    "        else:\n",
    "            updated_pair_freqs[pre_pair]-=count\n",
    "        updated_pair_freqs[new_pair]+=count\n",
    "    if pair_position < len(encoded_token)-2:\n",
    "        pos_token = encoded_token[pair_position+2]\n",
    "        pos_pair = (encoded_token[pair_position+1],pos_token)\n",
    "        new_pair = (new_index, pos_token)\n",
    "        if updated_pair_freqs[pos_pair] == count:\n",
    "            del updated_pair_freqs[pos_pair]\n",
    "        else:\n",
    "            updated_pair_freqs[pos_pair]-=count\n",
    "        updated_pair_freqs[new_pair]+=count\n",
    "    return updated_pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00aedfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9f8b098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "588f58eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_token)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81d6847f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a07e1817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 8, (3, 4): 3, (4, 2): 3, (3, 5): 2})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ebb58",
   "metadata": {},
   "source": [
    "## Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "9beef343",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"./tests/fixtures/tinystories_sample_5M.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "8c023152",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merges = train_bpe(\n",
    "        input_path=input_path,\n",
    "        vocab_size=1000,\n",
    "        special_tokens=[\"<|endoftext|>\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "6a428641",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_without_specials = [word for word in vocab.values() if word != b\"<|endoftext|>\"]\n",
    "for word_bytes in vocabs_without_specials:\n",
    "    assert b\"<|\" not in word_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "3b210bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "special_tokens=[\"<|endoftext|>\"]\n",
    "vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "3c16fbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_keys', 'vocab_values', 'merges'])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./tests/_snapshots/test_train_bpe_special_tokens.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "81b43e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (my_merge, target_merge) in enumerate(zip(merges,data['merges'])):\n",
    "    if not my_merge == target_merge:\n",
    "        print([i, my_merge, target_merge])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c5c52",
   "metadata": {},
   "source": [
    "## Q1.2 Problem (train_bpe_tinystories): BPE Training on TinyStories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032100f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdd53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8686e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"./data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "special_tokens = ['<|endoftext|>']\n",
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "233ee8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 356.20 MiB, increment: 23.55 MiB\n",
      "⏱️ 运行时间: 85.28 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "%memit vocab, merges = train_bpe(input_path, vocab_size, special_tokens)\n",
    "end_time = time.time()\n",
    "print(f\"⏱️ 运行时间: {end_time - start_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb145571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TinyStoriesV2-GPT4-train'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = r'./data/TinyStoriesV2-GPT4-train.txt'\n",
    "\n",
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ad74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
