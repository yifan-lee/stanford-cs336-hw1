{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from multiprocessing import Pool\n",
    "from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acb032",
   "metadata": {},
   "source": [
    "## Q1.1 Problem (train_bpe): BPE Tokenizer Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4108944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_element_counts(encoded_token_freqs: dict[str, int], pair: tuple[int, int], new_index: int) -> dict[str, int]:\n",
    "    updated_byte_level_counts = {}\n",
    "    for elements, counts in encoded_token_freqs.items():\n",
    "        new_element = []\n",
    "        elements_len = len(elements)\n",
    "        index = 0\n",
    "        while index <= elements_len-1:\n",
    "            if (index < elements_len-1) and (elements[index] == pair[0]) and (elements[index+1] == pair[1]):\n",
    "                new_element.append(new_index)\n",
    "                index += 2\n",
    "            else:\n",
    "                new_element.append(elements[index])\n",
    "                index += 1\n",
    "        new_byte_level_counts[tuple(new_element)] = counts\n",
    "    return new_byte_level_counts  \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pre_tokenize(string: str,special_tokens: list[str]) -> dict[str, int]:\n",
    "    string_list = split_by_special_tokens(string, special_tokens)\n",
    "    token_freqs = count_tokens(string_list)\n",
    "    return token_freqs\n",
    "\n",
    "def tokenize_chunk(args):\n",
    "    chunk_text, special_tokens = args\n",
    "    return pre_tokenize(chunk_text, special_tokens)\n",
    "\n",
    "def read_text_chunks(input_path: str, special_tokens: list[str], num_processes: int = 4) -> list[str]:\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        num_processes = 4\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "        chunks = []\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "            chunks.append((chunk, special_tokens))\n",
    "    return chunks\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    string = read_text_file(input_path)\n",
    "    token_freqs = pre_tokenize(string,special_tokens)\n",
    "    # num_processes = 4\n",
    "    # chunks = read_text_chunks(input_path, special_tokens, num_processes)\n",
    "    # with Pool(num_processes) as pool:\n",
    "    #     results = pool.map(tokenize_chunk, chunks)\n",
    "        \n",
    "    # token_freqs = defaultdict(int)\n",
    "    # for partial_count in results:\n",
    "    #     for word, count in partial_count.items():\n",
    "    #         token_freqs[word] += count\n",
    "            \n",
    "    encoded_token_freqs = encode_and_count_tokens(token_freqs)\n",
    "    vocab = build_initial_vocab(special_tokens)\n",
    "    vocab_len = len(vocab)\n",
    "    merges = []\n",
    "    while vocab_len<vocab_size:\n",
    "        pair_freqs = count_byte_pairs(encoded_token_freqs)\n",
    "        if len(pair_freqs) == 0:\n",
    "            break\n",
    "        pair = select_merge_pair(pair_freqs, vocab)\n",
    "        index1, index2 = pair\n",
    "        new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "        new_index = vocab_len\n",
    "        encoded_token_freqs = update_element_counts(encoded_token_freqs, pair,new_index)\n",
    "        merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "        vocab[new_index] = new_token\n",
    "        vocab_len+=1\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'\n",
    "\n",
    "epochs = 6\n",
    "vocab_size = 270\n",
    "string = \"hi. i'm yifan li. nice to meet you.<|endoftext|> this the what when here where\"\n",
    "input_path = r'./data/TinyStoriesV2-GPT4-valid.txt'\n",
    "string = read_text_file(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "308c9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 861.81 MiB, increment: 20.62 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7a435",
   "metadata": {},
   "source": [
    "## 个人优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "eb006187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(input_path: str) -> str:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_by_special_tokens(string: str, special_tokens: list[str]) -> list[str]:\n",
    "    pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "    return re.split(pattern,string)\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "def count_tokens(string_list: list[str]) -> dict[str, int]:\n",
    "    counts = defaultdict(int)\n",
    "    for s in string_list:\n",
    "        tokens = re.finditer(PAT, s)\n",
    "        for m in tokens:\n",
    "            tok = m.group(0)\n",
    "            counts[tok] += 1\n",
    "    return counts\n",
    "\n",
    "def encode_and_count_tokens(counts: dict[str, int])-> dict[str, int]:\n",
    "    encoded_token_freqs = defaultdict(int)\n",
    "    for token, count in counts.items():\n",
    "        elements = tuple(token.encode(\"utf-8\"))\n",
    "        encoded_token_freqs[elements] += count\n",
    "    return encoded_token_freqs\n",
    "\n",
    "def build_initial_vocab(special_tokens: list[str]) ->  dict[int, bytes]:\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    for i, tok in enumerate(special_tokens, start=256):\n",
    "        vocab[i] = tok.encode(\"utf-8\")\n",
    "    return vocab \n",
    "\n",
    "def get_byte_pairs_freqs_and_position(encoded_token_freqs: dict[str, int]) -> tuple[dict[tuple[int,int], int], dict[tuple[int,int], set[int]]]:\n",
    "    pair_freqs = defaultdict(int)\n",
    "    pair_position = defaultdict(set)\n",
    "    for index, (elements, count) in enumerate(encoded_token_freqs.items()):\n",
    "        for i in range(len(elements)-1):\n",
    "            pair_freqs[(elements[i],elements[i+1])] += count\n",
    "            pair_position[(elements[i],elements[i+1])].add(index)\n",
    "    return pair_freqs, pair_position\n",
    "\n",
    "def select_merge_pair(pair_freqs: dict[tuple[int,int], int], vocab:dict[int, bytes]) -> tuple[int, int]:\n",
    "    max_count = max(pair_freqs.values())\n",
    "    candidate_pairs = [key for key, value in pair_freqs.items() if value == max_count]\n",
    "    def sort_pair(pair):\n",
    "        index1, index2 = pair\n",
    "        return(vocab[index1], vocab[index2])\n",
    "    pair = max(candidate_pairs, key = sort_pair)\n",
    "    return pair\n",
    "\n",
    "def find_subtuple_index(sequence: tuple, subseq: tuple) -> list[int]:\n",
    "    position = []\n",
    "    subseq_len = len(subseq)\n",
    "    for i in range(len(sequence)-subseq_len+1):\n",
    "        if sequence[i:i+subseq_len] == subseq:\n",
    "            position.append(i)\n",
    "    return position\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_encoded_token(encoded_token, pair, new_index):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(encoded_token):\n",
    "        if i < len(encoded_token) - 1 and (encoded_token[i], encoded_token[i + 1]) == pair:\n",
    "            result.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(encoded_token[i])\n",
    "            i += 1\n",
    "    return tuple(result)\n",
    "\n",
    "def remove_or_decrement_pair(pair_freqs: dict[tuple[int,int], int], pair: tuple[int,int], count: int):\n",
    "    updated_pair_freqs = pair_freqs.copy()\n",
    "    if updated_pair_freqs[pair] == count:\n",
    "        del updated_pair_freqs[pair]\n",
    "    else:\n",
    "        updated_pair_freqs[pair] -= count\n",
    "    return updated_pair_freqs\n",
    "\n",
    "def update_encoded_token_freqs(encoded_token_freqs: dict[list[int],int],encoded_token_map: dict[list[int], list[int]]) -> dict[list[int],int]:\n",
    "    updated_encoded_token_freqs = {}\n",
    "    for i, (key,value) in enumerate(encoded_token_freqs.items()):\n",
    "        if key in encoded_token_map:\n",
    "            new_key = encoded_token_map[key]\n",
    "            updated_encoded_token_freqs[new_key] = value\n",
    "        else:\n",
    "            updated_encoded_token_freqs[key] = value\n",
    "    return updated_encoded_token_freqs\n",
    "\n",
    "def initiate_update(x, pair):\n",
    "    y = x.copy()\n",
    "    del y[pair]\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def update_all_structures(pair_positions, pair_freqs, encoded_token_freqs, pair, new_index):\n",
    "    updated_pair_positions = initiate_update(pair_positions,pair)\n",
    "    updated_pair_freqs = initiate_update(pair_freqs,pair)\n",
    "\n",
    "    encoded_token_map = {}\n",
    "    positions = pair_positions[pair]\n",
    "    for position in positions:\n",
    "        encoded_token, count = list(encoded_token_freqs.items())[position]\n",
    "        positions = find_subtuple_index(encoded_token,pair)\n",
    "        encoded_token_map[encoded_token] = update_encoded_token(encoded_token,positions, new_index)\n",
    "        for pos in positions:\n",
    "            if pos > 0:\n",
    "                pre_token = encoded_token[pos-1]\n",
    "                old_pair = (pre_token,encoded_token[pos])\n",
    "                new_pair = (pre_token, new_index)\n",
    "                updated_pair_freqs = remove_or_decrement_pair(updated_pair_freqs, old_pair, count)\n",
    "                updated_pair_freqs[new_pair]+=count\n",
    "                updated_pair_positions[new_pair].add(position)\n",
    "            if pos < len(encoded_token)-2:\n",
    "                pos_token = encoded_token[pos+2]\n",
    "                old_pair = (encoded_token[pos+1],pos_token)\n",
    "                new_pair = (new_index, pos_token)\n",
    "                updated_pair_freqs = remove_or_decrement_pair(updated_pair_freqs, old_pair, count)\n",
    "                updated_pair_freqs[new_pair]+=count\n",
    "                updated_pair_positions[new_pair].add(position)\n",
    "\n",
    "    updated_encoded_token_freqs = update_encoded_token_freqs(encoded_token_freqs,encoded_token_map)\n",
    "    return updated_pair_positions, updated_pair_freqs, updated_encoded_token_freqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0df35532",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = r'./data/test.txt'\n",
    "vocab_size = 260\n",
    "string = \"hi. i'm yifan li. nice to meet you.<|endoftext|> this the what when here where\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a00f3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = split_by_special_tokens(string, special_tokens)\n",
    "token_freqs = count_tokens(string_list)\n",
    "encoded_token_freqs = encode_and_count_tokens(token_freqs)\n",
    "vocab = build_initial_vocab(special_tokens)\n",
    "vocab_len = len(vocab)\n",
    "merges = []\n",
    "\n",
    "pair_freqs = count_byte_pairs(encoded_token_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e575e61",
   "metadata": {},
   "source": [
    "### Update merge logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e58262fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_initial_vocab(special_tokens)\n",
    "vocab_len = len(vocab)\n",
    "merges = []\n",
    "\n",
    "encoded_token_freqs = defaultdict(int)\n",
    "encoded_token_freqs[(1,2,3,4,2,3)] = 3\n",
    "encoded_token_freqs[(2,3,5)] = 2\n",
    "pair_freqs,pair_positions = get_byte_pairs_freqs_and_position(encoded_token_freqs)\n",
    "pair = select_merge_pair(pair_freqs, vocab)\n",
    "\n",
    "index1, index2 = pair\n",
    "new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "new_index = vocab_len\n",
    "vocab[new_index] = new_token\n",
    "vocab_len+=1\n",
    "merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "\n",
    "\n",
    "# pair_freqs = update_pair_freqs_after_merge(pair_freqs, positions, encoded_token_freqs, pair, new_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243a2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4, 2, 3)\n",
      "(2, 3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[defaultdict(int, {(1, 257): 3, (257, 4): 3, (4, 257): 3, (257, 5): 2}),\n",
       " {(1, 2, 3, 4, 2, 3): 3, (2, 3, 5): 2},\n",
       " defaultdict(set,\n",
       "             {(1, 2): {0},\n",
       "              (3, 4): {0},\n",
       "              (4, 2): {0},\n",
       "              (3, 5): {1},\n",
       "              (1, 257): {0},\n",
       "              (257, 4): {0},\n",
       "              (4, 257): {0},\n",
       "              (257, 5): {1}})]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_pair_positions = initiate_update(pair_positions,pair)\n",
    "updated_pair_freqs = initiate_update(pair_freqs,pair)\n",
    "\n",
    "encoded_token_map = {}\n",
    "positions = pair_positions[pair]\n",
    "for position in positions:\n",
    "    encoded_token, count = list(encoded_token_freqs.items())[position]\n",
    "    positions = find_subtuple_index(encoded_token,pair)\n",
    "    encoded_token_map[encoded_token] = update_encoded_token(encoded_token,positions, new_index)\n",
    "    for pos in positions:\n",
    "        if pos > 0:\n",
    "            pre_token = encoded_token[pos-1]\n",
    "            old_pair = (pre_token,encoded_token[pos])\n",
    "            new_pair = (pre_token, new_index)\n",
    "            updated_pair_freqs = remove_or_decrement_pair(updated_pair_freqs, old_pair, count)\n",
    "            updated_pair_freqs[new_pair]+=count\n",
    "            updated_pair_positions[new_pair].add(position)\n",
    "        if pos < len(encoded_token)-2:\n",
    "            pos_token = encoded_token[pos+2]\n",
    "            old_pair = (encoded_token[pos+1],pos_token)\n",
    "            new_pair = (new_index, pos_token)\n",
    "            updated_pair_freqs = remove_or_decrement_pair(updated_pair_freqs, old_pair, count)\n",
    "            updated_pair_freqs[new_pair]+=count\n",
    "            updated_pair_positions[new_pair].add(position)\n",
    "\n",
    "updated_encoded_token_freqs = update_encoded_token_freqs(encoded_token_freqs,encoded_token_map)\n",
    "[updated_pair_freqs,updated_encoded_token_freqs,updated_pair_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5daf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 3, 4, 2, 3): (1, 2, 3, 4, 2, 3), (2, 3, 5): (2, 3, 5)}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_encoded_token!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ef7e2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 0\n",
    "encoded_token, count = list(encoded_token_freqs.items())[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "783c9e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 257, 4, 257)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_encoded_token(encoded_token, pair, new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4700a25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2, 3, 4, 2, 3): (1, 2, 3, 4, 2, 3), (2, 3, 5): (2, 3, 5)}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_encoded_token(encoded_token, pair, new_index):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(encoded_token):\n",
    "        if i < len(encoded_token) - 1 and (encoded_token[i], encoded_token[i + 1]) == pair:\n",
    "            result.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(encoded_token[i])\n",
    "            i += 1\n",
    "    return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b1603452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4, 2, 3)\n",
      "(1, 2, 3, 4, 2, 3)\n",
      "(2, 3, 5)\n",
      "(2, 3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(1, 2, 3, 4, 2, 3): 3, (2, 3, 5): 2}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_encoded_token_freqs = {}\n",
    "for i, (key,value) in enumerate(encoded_token_freqs.items()):\n",
    "    if key in encoded_token_map:\n",
    "        print(key)\n",
    "        new_key = encoded_token_map[key]\n",
    "        print(new_key)\n",
    "        updated_encoded_token_freqs[new_key] = value\n",
    "    else:\n",
    "        updated_encoded_token_freqs[key] = value\n",
    "    # print(updated_encoded_token_freqs)\n",
    "updated_encoded_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6523b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_encoded_token_freqs(encoded_token_freqs: dict[list[int],int],encoded_token_map: dict[list[int], list[int]]) -> dict[list[int],int]:\n",
    "    updated_encoded_token_freqs = {}\n",
    "    for i, (key,value) in enumerate(encoded_token_freqs.items()):\n",
    "        if key in encoded_token_map:\n",
    "            new_key = encoded_token_map[key]\n",
    "            updated_encoded_token_freqs[new_key] = value\n",
    "        else:\n",
    "            updated_encoded_token_freqs[key] = value\n",
    "    return updated_encoded_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83457787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int, {(1, 257): 3, (257, 4): 3, (4, 257): 3, (257, 5): 2}),\n",
       " {(1, 2, 3, 4, 2, 3): 3, (2, 3, 5): 2},\n",
       " defaultdict(set,\n",
       "             {(1, 2): {0},\n",
       "              (3, 4): {0},\n",
       "              (4, 2): {0},\n",
       "              (3, 5): {1},\n",
       "              (1, 257): {0},\n",
       "              (257, 4): {0},\n",
       "              (4, 257): {0},\n",
       "              (257, 5): {1}})]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "updated_pair_positions, updated_pair_freqs, updated_encoded_token_freqs = update_all_structures(pair_positions, pair_freqs, encoded_token_freqs, pair, new_index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962e4e6",
   "metadata": {},
   "source": [
    "**updated_encoded_token_freqs is not correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "82cdc1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 257, 4, 257): 3, (257, 5): 2}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "295fa252",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 0\n",
    "encoded_token, count = list(encoded_token_freqs.items())[position]\n",
    "pair_positions = find_subtuple_index(encoded_token,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7980249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 257, 4, 257)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ae472dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_to_keys(A: dict, positions: list[int], F):\n",
    "    keys = list(A.keys())  # 保持 key 的顺序\n",
    "    for pos in positions:\n",
    "        old_key = keys[pos]\n",
    "        value = A.pop(old_key)         # 先取出旧值并删除旧 key\n",
    "        new_key = F(old_key)           # 使用函数 F 得到新 key\n",
    "        A[new_key] = value   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "08e48d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('b', 'c'): 2, ('A', 'B'): 1, ('C', 'D'): 3}\n"
     ]
    }
   ],
   "source": [
    "A = {('a', 'b'): 1, ('b', 'c'): 2, ('c', 'd'): 3}\n",
    "positions = [0, 2]\n",
    "\n",
    "# 假设你想把 key 的每个字符都变成大写\n",
    "def F(key):\n",
    "    return tuple(k.upper() for k in key)\n",
    "\n",
    "apply_function_to_keys(A, positions, F)\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76d05a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3, 4, 2, 3), 3]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_token, count = list(encoded_token_freqs.items())[position]\n",
    "[encoded_token,count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb687c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_positions = find_subtuple_index(encoded_token,pair)\n",
    "pair_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d2c62cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 8, (3, 4): 3, (4, 2): 3, (3, 5): 2})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004dace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_pair_freqs  = pair_freqs.copy()\n",
    "updated_pair_freqs[pair] -= count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30091f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 5, (3, 4): 3, (3, 5): 2, (4, 257): 3})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_pair_freqs = pair_freqs.copy()\n",
    "for pair_position in pair_positions:\n",
    "    updated_pair_freqs = update_pair_freqs(pair_freqs, encoded_token, pair_position, count)\n",
    "updated_pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f65581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 8, (3, 4): 3, (4, 2): 3, (3, 5): 2})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_pair_freqs(pair_freqs: dict[tuple[int, int], int], encoded_token: tuple[int], pair_position: int, count: int) -> dict[tuple[int, int], int]:\n",
    "    updated_pair_freqs  = pair_freqs.copy()\n",
    "    updated_pair_freqs[pair] -= count\n",
    "    if pair_position > 0:\n",
    "        pre_token = encoded_token[pair_position-1]\n",
    "        pre_pair = (pre_token,encoded_token[pair_position])\n",
    "        new_pair = (pre_token, new_index)\n",
    "        if updated_pair_freqs[pre_pair] == count:\n",
    "            del updated_pair_freqs[pre_pair]\n",
    "        else:\n",
    "            updated_pair_freqs[pre_pair]-=count\n",
    "        updated_pair_freqs[new_pair]+=count\n",
    "    if pair_position < len(encoded_token)-2:\n",
    "        pos_token = encoded_token[pair_position+2]\n",
    "        pos_pair = (encoded_token[pair_position+1],pos_token)\n",
    "        new_pair = (new_index, pos_token)\n",
    "        if updated_pair_freqs[pos_pair] == count:\n",
    "            del updated_pair_freqs[pos_pair]\n",
    "        else:\n",
    "            updated_pair_freqs[pos_pair]-=count\n",
    "        updated_pair_freqs[new_pair]+=count\n",
    "    return updated_pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00aedfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9f8b098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "588f58eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_token)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81d6847f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a07e1817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, 2): 3, (2, 3): 8, (3, 4): 3, (4, 2): 3, (3, 5): 2})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ebb58",
   "metadata": {},
   "source": [
    "## Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "9beef343",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"./tests/fixtures/tinystories_sample_5M.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "8c023152",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merges = train_bpe(\n",
    "        input_path=input_path,\n",
    "        vocab_size=1000,\n",
    "        special_tokens=[\"<|endoftext|>\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "6a428641",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_without_specials = [word for word in vocab.values() if word != b\"<|endoftext|>\"]\n",
    "for word_bytes in vocabs_without_specials:\n",
    "    assert b\"<|\" not in word_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "3b210bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "special_tokens=[\"<|endoftext|>\"]\n",
    "vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "3c16fbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_keys', 'vocab_values', 'merges'])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./tests/_snapshots/test_train_bpe_special_tokens.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "81b43e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (my_merge, target_merge) in enumerate(zip(merges,data['merges'])):\n",
    "    if not my_merge == target_merge:\n",
    "        print([i, my_merge, target_merge])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c5c52",
   "metadata": {},
   "source": [
    "## Q1.2 Problem (train_bpe_tinystories): BPE Training on TinyStories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032100f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdd53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8686e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"./data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "special_tokens = ['<|endoftext|>']\n",
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "233ee8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 356.20 MiB, increment: 23.55 MiB\n",
      "⏱️ 运行时间: 85.28 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "%memit vocab, merges = train_bpe(input_path, vocab_size, special_tokens)\n",
    "end_time = time.time()\n",
    "print(f\"⏱️ 运行时间: {end_time - start_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb145571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TinyStoriesV2-GPT4-train'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = r'./data/TinyStoriesV2-GPT4-train.txt'\n",
    "\n",
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ad74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
