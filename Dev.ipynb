{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from multiprocessing import Pool\n",
    "from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acb032",
   "metadata": {},
   "source": [
    "## Q1.1 Problem (train_bpe): BPE Tokenizer Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4108944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_txt_as_str(input_path: str) -> str:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_string(string: str, special_tokens: list[str]) -> list[str]:\n",
    "    pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "    return re.split(pattern,string)\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "def get_tok_counts(string_list: list[str]) -> dict[str, int]:\n",
    "    counts = defaultdict(int)\n",
    "    for s in string_list:\n",
    "        tokens = re.finditer(PAT, s)\n",
    "        for m in tokens:\n",
    "            tok = m.group(0)\n",
    "            counts[tok] += 1\n",
    "    return counts\n",
    "\n",
    "def get_byte_counts(counts: dict[str, int])-> dict[str, int]:\n",
    "    element_counts = defaultdict(int)\n",
    "    for token, count in counts.items():\n",
    "        elements = tuple(token.encode(\"utf-8\"))\n",
    "        element_counts[elements] += count\n",
    "    return element_counts\n",
    "\n",
    "def get_pair_counts(element_counts: dict[str, int]) -> dict[tuple[int,int], int]:\n",
    "    pair_counts = defaultdict(int)\n",
    "    for elements, count in element_counts.items():\n",
    "        for i in range(len(elements)-1):\n",
    "            pair_counts[(elements[i],elements[i+1])] += count\n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "def update_element_counts(byte_level_counts: dict[str, int], pair: tuple[int, int], new_index: int) -> dict[str, int]:\n",
    "    new_byte_level_counts = {}\n",
    "    for elements, counts in byte_level_counts.items():\n",
    "        new_element = []\n",
    "        elements_len = len(elements)\n",
    "        index = 0\n",
    "        while index <= elements_len-1:\n",
    "            if (index < elements_len-1) and (elements[index] == pair[0]) and (elements[index+1] == pair[1]):\n",
    "                new_element.append(new_index)\n",
    "                index += 2\n",
    "            else:\n",
    "                new_element.append(elements[index])\n",
    "                index += 1\n",
    "        new_byte_level_counts[tuple(new_element)] = counts\n",
    "    return new_byte_level_counts  \n",
    "\n",
    "def initiate_vocab(special_tokens: list[str]) ->  dict[int, bytes]:\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    for i, tok in enumerate(special_tokens, start=256):\n",
    "        vocab[i] = tok.encode(\"utf-8\")\n",
    "    return vocab   \n",
    "\n",
    "def find_max_pair(pair_counts: dict[tuple[int,int], int], vocab:dict[int, bytes]) -> tuple[int, int]:\n",
    "    max_count = max(pair_counts.values())\n",
    "    candidate_pairs = [key for key, value in pair_counts.items() if value == max_count]\n",
    "    def sort_pair(pair):\n",
    "        index1, index2 = pair\n",
    "        return(vocab[index1], vocab[index2])\n",
    "    pair = max(candidate_pairs, key = sort_pair)\n",
    "    return pair\n",
    "\n",
    "\n",
    "def pre_tokenize(string: str,special_tokens: list[str]) -> dict[str, int]:\n",
    "    string_list = split_string(string, special_tokens)\n",
    "    word_level_counts = get_tok_counts(string_list)\n",
    "    return word_level_counts\n",
    "\n",
    "def process_chunk(args):\n",
    "    chunk_text, special_tokens = args\n",
    "    return pre_tokenize(chunk_text, special_tokens)\n",
    "\n",
    "def load_txt_in_chunks(input_path: str, special_tokens: list[str], num_processes: int = 4) -> list[str]:\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        num_processes = 4\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "        chunks = []\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "            chunks.append((chunk, special_tokens))\n",
    "    return chunks\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    string = load_txt_as_str(input_path)\n",
    "    word_level_counts = pre_tokenize(string,special_tokens)\n",
    "    # num_processes = 4\n",
    "    # chunks = load_txt_in_chunks(input_path, special_tokens, num_processes)\n",
    "    # with Pool(num_processes) as pool:\n",
    "    #     results = pool.map(process_chunk, chunks)\n",
    "        \n",
    "    # word_level_counts = defaultdict(int)\n",
    "    # for partial_count in results:\n",
    "    #     for word, count in partial_count.items():\n",
    "    #         word_level_counts[word] += count\n",
    "            \n",
    "    byte_level_counts = get_byte_counts(word_level_counts)\n",
    "    vocab = initiate_vocab(special_tokens)\n",
    "    vocab_len = len(vocab)\n",
    "    merges = []\n",
    "    while vocab_len<vocab_size:\n",
    "        pair_counts = get_pair_counts(byte_level_counts)\n",
    "        if len(pair_counts) == 0:\n",
    "            break\n",
    "        pair = find_max_pair(pair_counts, vocab)\n",
    "        index1, index2 = pair\n",
    "        new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "        new_index = vocab_len\n",
    "        byte_level_counts = update_element_counts(byte_level_counts, pair,new_index)\n",
    "        merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "        vocab[new_index] = new_token\n",
    "        vocab_len+=1\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "4e4f25a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'\n",
    "\n",
    "epochs = 6\n",
    "vocab_size = 270\n",
    "string = \"hi. i'm yifan li. nice to meet you.<|endoftext|> this the what when here where\"\n",
    "input_path = r'./data/TinyStoriesV2-GPT4-valid.txt'\n",
    "string = load_txt_as_str(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 861.81 MiB, increment: 20.62 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ebb58",
   "metadata": {},
   "source": [
    "## Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "9beef343",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"./tests/fixtures/tinystories_sample_5M.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "8c023152",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merges = train_bpe(\n",
    "        input_path=input_path,\n",
    "        vocab_size=1000,\n",
    "        special_tokens=[\"<|endoftext|>\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "6a428641",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_without_specials = [word for word in vocab.values() if word != b\"<|endoftext|>\"]\n",
    "for word_bytes in vocabs_without_specials:\n",
    "    assert b\"<|\" not in word_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "3b210bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "special_tokens=[\"<|endoftext|>\"]\n",
    "vocab, merges = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "3c16fbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_keys', 'vocab_values', 'merges'])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./tests/_snapshots/test_train_bpe_special_tokens.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "81b43e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (my_merge, target_merge) in enumerate(zip(merges,data['merges'])):\n",
    "    if not my_merge == target_merge:\n",
    "        print([i, my_merge, target_merge])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c5c52",
   "metadata": {},
   "source": [
    "## Q1.2 Problem (train_bpe_tinystories): BPE Training on TinyStories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032100f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdd53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8686e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"./data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "special_tokens = ['<|endoftext|>']\n",
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "233ee8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 356.20 MiB, increment: 23.55 MiB\n",
      "⏱️ 运行时间: 85.28 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "%memit vocab, merges = train_bpe(input_path, vocab_size, special_tokens)\n",
    "end_time = time.time()\n",
    "print(f\"⏱️ 运行时间: {end_time - start_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb145571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
