{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import regex as re\n",
    "# from multiprocessing import Pool\n",
    "# from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "# from memory_profiler import profile\n",
    "# import time, tracemalloc\n",
    "# from dataclasses import dataclass\n",
    "# from typing import BinaryIO, Iterable, Iterator\n",
    "# import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, einsum, reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acb032",
   "metadata": {},
   "source": [
    "## Q2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "11d6a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn(64, 128, 128, 3) # (batch, height, width, channel)\n",
    "dim_by = torch.linspace(start=0.0, end=1.0, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "be1b65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_value = rearrange(dim_by, \"dim_value -> 1 dim_value 1 1 1\")\n",
    "images_rearr = rearrange(images, \"b height width channel -> b 1 height width channel\")\n",
    "dimmed_images = images_rearr * dim_value\n",
    "dimmed_images = einsum(\n",
    "    images, dim_by,\n",
    "    \"batch height width channel, dim_value -> batch dim_value height width channel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "ce9da0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 128, 128, 3])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimmed_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541b3d4",
   "metadata": {},
   "source": [
    "# Q 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "d12318ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        super().__init__()\n",
    "        ## Construct a linear transformation module. This function should accept the following parameters:\n",
    "        self.in_features = in_features ## final dimension of the input\n",
    "        self.out_features = out_features ## final dimension of the output\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        w = torch.empty(in_features, out_features)\n",
    "        std = torch.sqrt(torch.tensor(2.0/(in_features+out_features)))\n",
    "        self.weight = nn.Parameter(nn.init.trunc_normal_(w, mean=0.0, std=std.item(),a=-3*std.item(),b=3*std.item()))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Apply the linear transformation to the input\n",
    "        output = einsum(\n",
    "            self.weight, x,\n",
    "            \"in_dim out_dim, in_dim -> out_dim\"\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b05a99",
   "metadata": {},
   "source": [
    "# Q 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "fcb550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, device: torch.device | None = None, dtype: torch.dtype | None = None):\n",
    "        ## Construct an embedding module\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings ## Size of the vocabulary\n",
    "        self.embedding_dim = embedding_dim ## Dimension of the embedding vectors\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "        \n",
    "        w = torch.empty(num_embeddings, embedding_dim)\n",
    "        std = 1.0\n",
    "        self.weight = nn.Parameter(nn.init.trunc_normal_(w, mean=0.0, std=std,a=-3,b=3))\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        ## Lookup the embedding vectors for the given token IDs.\n",
    "        return self.weight[token_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06587e",
   "metadata": {},
   "source": [
    "# Q 3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31176275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None):\n",
    "        ## Construct the RMSNorm module.\n",
    "        super().__init__()\n",
    "        self.d_model = d_model ## Hidden dimension of the model\n",
    "        self.eps = eps ## Epsilon value for numerical stability\n",
    "        self.device = device ## Device to store the parameters on\n",
    "        self.dtype = dtype ## Data type of the parameters\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(d_model))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## Process an input tensor of shape\n",
    "        \n",
    "        in_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        x_squaremean = reduce(\n",
    "            x^2, \"... d_model -> ... 1\", 'mean'\n",
    "        )\n",
    "        x_RMS = (x_squaremean+self.eps).sqrt()\n",
    "        result = x / x_RMS * self.weights\n",
    "        return result.to(in_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "bdfda749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from support.transformer import RMSNorm\n",
    "\n",
    "# 超参数\n",
    "d_model = 4\n",
    "eps = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "21088daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3636, -0.3152,  0.8715, -1.3661],\n",
       "        [-0.8370, -0.5851, -0.4513, -1.0230]])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造输入\n",
    "x = torch.randn(2, d_model)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "77ac0aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化 RMSNorm\n",
    "rmsnorm = RMSNorm(d_model, eps)\n",
    "rmsnorm.weights.data.fill_(1.0)  # gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "16147faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向计算\n",
    "y = rmsnorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "b99ced40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3204, -0.2778,  0.7680, -1.2039],\n",
       "        [-0.7376, -0.5157, -0.3977, -0.9015]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "9ae5a520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8370, -0.5851, -0.4513, -1.0230])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "01520e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1055, -0.7728, -0.5960, -1.3511])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]/torch.sqrt((x[1]**2).sum()/d_model+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 检查每个样本的 RMS\n",
    "rms = torch.sqrt(torch.mean(y**2, dim=-1))\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Output:\", y)\n",
    "print(\"RMS per sample:\", rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b60f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b2b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
