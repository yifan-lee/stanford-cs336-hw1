{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd98ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from multiprocessing import Pool\n",
    "from support.find_chunk_boundaries import find_chunk_boundaries\n",
    "from memory_profiler import profile\n",
    "import time, tracemalloc\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4acb032",
   "metadata": {},
   "source": [
    "## Q1.1 Problem (train_bpe): BPE Tokenizer Training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745391d",
   "metadata": {},
   "source": [
    "### Version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c19cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import regex as re\n",
    "\n",
    "\n",
    "def load_txt_as_str(input_path: str) -> str:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_string(string: str, special_tokens: list[str]) -> list[str]:\n",
    "    pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "    return re.split(pattern,string)\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "def get_tok_counts(string_list: list[str]) -> dict[str, int]:\n",
    "    counts = defaultdict(int)\n",
    "    for s in string_list:\n",
    "        tokens = re.finditer(PAT, s)\n",
    "        for m in tokens:\n",
    "            tok = m.group(0)\n",
    "            counts[tok] += 1\n",
    "    return counts\n",
    "\n",
    "def get_byte_counts(counts: dict[str, int])-> dict[str, int]:\n",
    "    element_counts = defaultdict(int)\n",
    "    for token, count in counts.items():\n",
    "        elements = tuple(token.encode(\"utf-8\"))\n",
    "        element_counts[elements] += count\n",
    "    return element_counts\n",
    "\n",
    "def get_pair_counts(element_counts: dict[str, int]) -> dict[tuple[int,int], int]:\n",
    "    pair_counts = defaultdict(int)\n",
    "    for elements, count in element_counts.items():\n",
    "        for i in range(len(elements)-1):\n",
    "            pair_counts[(elements[i],elements[i+1])] += count\n",
    "    return pair_counts\n",
    "\n",
    "\n",
    "def update_element_counts(byte_level_counts: dict[str, int], pair: tuple[int, int], new_index: int) -> dict[str, int]:\n",
    "    new_byte_level_counts = {}\n",
    "    for elements, counts in byte_level_counts.items():\n",
    "        new_element = []\n",
    "        elements_len = len(elements)\n",
    "        index = 0\n",
    "        while index <= elements_len-1:\n",
    "            if (index < elements_len-1) and (elements[index] == pair[0]) and (elements[index+1] == pair[1]):\n",
    "                new_element.append(new_index)\n",
    "                index += 2\n",
    "            else:\n",
    "                new_element.append(elements[index])\n",
    "                index += 1\n",
    "        new_byte_level_counts[tuple(new_element)] = counts\n",
    "    return new_byte_level_counts  \n",
    "\n",
    "def initiate_vocab(special_tokens: list[str]) ->  dict[int, bytes]:\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    for i, tok in enumerate(special_tokens, start=256):\n",
    "        vocab[i] = tok.encode(\"utf-8\")\n",
    "    return vocab   \n",
    "\n",
    "def find_max_pair(pair_counts: dict[tuple[int,int], int], vocab:dict[int, bytes]) -> tuple[int, int]:\n",
    "    max_count = max(pair_counts.values())\n",
    "    candidate_pairs = [key for key, value in pair_counts.items() if value == max_count]\n",
    "    def sort_pair(pair):\n",
    "        index1, index2 = pair\n",
    "        return(vocab[index1], vocab[index2])\n",
    "    pair = max(candidate_pairs, key = sort_pair)\n",
    "    return pair\n",
    "\n",
    "\n",
    "def pre_tokenize(string: str,vocab_size: int,special_tokens: list[str]) -> tuple[dict[int, bytes],list[tuple[bytes, bytes]]]:\n",
    "    merges = []\n",
    "    string_list = split_string(string, special_tokens)\n",
    "    word_level_counts = get_tok_counts(string_list)\n",
    "    byte_level_counts = get_byte_counts(word_level_counts)\n",
    "    vocab = initiate_vocab(special_tokens)\n",
    "    vocab_len = len(vocab)\n",
    "\n",
    "    while vocab_len<vocab_size:\n",
    "        pair_counts = get_pair_counts(byte_level_counts)\n",
    "        if len(pair_counts) == 0:\n",
    "            break\n",
    "        pair = find_max_pair(pair_counts, vocab)\n",
    "        index1, index2 = pair\n",
    "        new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "        new_index = vocab_len\n",
    "        byte_level_counts = update_element_counts(byte_level_counts, pair,new_index)\n",
    "        merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "        vocab[new_index] = new_token\n",
    "        vocab_len+=1\n",
    "    return vocab, merges\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    string = load_txt_as_str(input_path)\n",
    "    vocab, merges = pre_tokenize(string, vocab_size,special_tokens)\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4388b",
   "metadata": {},
   "source": [
    "### Version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pair = tuple[int,int]\n",
    "Encoded_Token = tuple[int, ...]\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705db8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class TokenMergePlan():\n",
    "    old_token: Encoded_Token\n",
    "    new_token: Encoded_Token\n",
    "    count: int\n",
    "    pair_positions: list[int]\n",
    "\n",
    "class PairFreqsDelta():\n",
    "    inc: defaultdict[Pair, int]\n",
    "    inc: defaultdict[Pair, int]\n",
    "    def __init__(self):\n",
    "        self.inc = defaultdict(int)\n",
    "        self.dec = defaultdict(int)\n",
    "\n",
    "class PairInhereitDelta():\n",
    "    add: defaultdict[Pair,set[Encoded_Token]]\n",
    "    remove: defaultdict[Pair,set[Encoded_Token]]\n",
    "    def __init__(self):\n",
    "        self.add = defaultdict(set[Encoded_Token])\n",
    "        self.remove = defaultdict(set[Encoded_Token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68954832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(input_path: str) -> str:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_by_special_tokens(string: str, special_tokens: list[str]) -> list[str]:\n",
    "    pattern = \"|\".join(re.escape(tok) for tok in special_tokens)\n",
    "    return re.split(pattern,string)\n",
    "\n",
    "def count_tokens(string_list: list[str]) -> dict[str, int]:\n",
    "    counts = defaultdict(int)\n",
    "    for s in string_list:\n",
    "        tokens = re.finditer(PAT, s)\n",
    "        for m in tokens:\n",
    "            tok = m.group(0)\n",
    "            counts[tok] += 1\n",
    "    return counts\n",
    "\n",
    "def encode_and_count_tokens(counts: dict[str, int])-> dict[Encoded_Token, int]:\n",
    "    encoded_token_freqs = defaultdict(int)\n",
    "    for token, count in counts.items():\n",
    "        elements = tuple(token.encode(\"utf-8\"))\n",
    "        encoded_token_freqs[elements] += count\n",
    "    return encoded_token_freqs\n",
    "\n",
    "def build_initial_vocab(special_tokens: list[str]) ->  dict[int, bytes]:\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    for i, tok in enumerate(special_tokens, start=256):\n",
    "        vocab[i] = tok.encode(\"utf-8\")\n",
    "    return vocab\n",
    "\n",
    "def get_byte_pairs(encoded_token_freqs: dict[Encoded_Token, int]) -> dict[Pair, int]:\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for tok, count in encoded_token_freqs.items():\n",
    "        for i in range(len(tok)-1):\n",
    "            pair_freqs[(tok[i],tok[i+1])] += count\n",
    "    return pair_freqs\n",
    "\n",
    "def get_byte_pairs_inhereit(encoded_token_freqs: dict[Encoded_Token, int]) -> dict[Pair, set[Encoded_Token]]:\n",
    "    pair_inhereit = defaultdict(set)\n",
    "    for tok, count in encoded_token_freqs.items():\n",
    "        for i in range(len(tok)-1):\n",
    "            pair_inhereit[(tok[i],tok[i+1])].add(tok)\n",
    "    return pair_inhereit\n",
    "\n",
    "def select_merge_pair(pair_freqs: dict[Pair, int], vocab:dict[int, bytes]) -> Pair:\n",
    "    max_count = max(pair_freqs.values())\n",
    "    candidate_pairs = [key for key, value in pair_freqs.items() if value == max_count]\n",
    "    def sort_pair(pair):\n",
    "        index1, index2 = pair\n",
    "        return(vocab[index1], vocab[index2])\n",
    "    pair = max(candidate_pairs, key = sort_pair)\n",
    "    return pair\n",
    "\n",
    "def update_encoded_token(encoded_token: Encoded_Token, pair: Pair, new_index: int) -> Encoded_Token:\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(encoded_token):\n",
    "        if i < len(encoded_token) - 1 and (encoded_token[i], encoded_token[i + 1]) == pair:\n",
    "            result.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            result.append(encoded_token[i])\n",
    "            i += 1\n",
    "    return tuple(result)\n",
    "\n",
    "def find_subtuple_index(sequence: tuple, subseq: tuple) -> list[int]:\n",
    "    position = []\n",
    "    subseq_len = len(subseq)\n",
    "    for i in range(len(sequence)-subseq_len+1):\n",
    "        if sequence[i:i+subseq_len] == subseq:\n",
    "            position.append(i)\n",
    "    return position\n",
    "\n",
    "def remove_or_decrement_pair(pair_freqs: dict[Pair, int], pair: Pair, count: int) -> dict[Pair, int]:\n",
    "    updated_pair_freqs = pair_freqs.copy()\n",
    "    if updated_pair_freqs[pair] == count:\n",
    "        del updated_pair_freqs[pair]\n",
    "    else:\n",
    "        updated_pair_freqs[pair] -= count\n",
    "    return updated_pair_freqs\n",
    "\n",
    "def build_merge_plan(tok_need_update: set[Encoded_Token], encoded_token_freqs: dict[Encoded_Token, int], pair: Pair, new_index: int) -> list[TokenMergePlan]:\n",
    "    plan = []\n",
    "    for encoded_token in tok_need_update:\n",
    "        new_encoded_token = update_encoded_token(encoded_token, pair, new_index)\n",
    "        count = encoded_token_freqs[encoded_token]\n",
    "        pair_positions = find_subtuple_index(encoded_token,pair)\n",
    "        plan.append(TokenMergePlan(encoded_token,new_encoded_token,count,pair_positions))\n",
    "    return plan\n",
    "\n",
    "def update_encoded_token_freqs(plan: list[TokenMergePlan], encoded_token_freqs: dict[Encoded_Token, int]) ->  dict[Encoded_Token, int]:\n",
    "    new_encoded_token_freqs = encoded_token_freqs.copy()\n",
    "    for item in plan:\n",
    "        del new_encoded_token_freqs[item.old_token]\n",
    "        new_encoded_token_freqs[item.new_token] = item.count\n",
    "    return new_encoded_token_freqs\n",
    "\n",
    "def compute_freqs_deltas(plan: list[TokenMergePlan], new_index:int) -> PairFreqsDelta:\n",
    "    pair_freqs_d = PairFreqsDelta()\n",
    "    for item in plan:\n",
    "        old_token = item.old_token\n",
    "        count = item.count\n",
    "        for pos in item.pair_positions:\n",
    "            if pos > 0:\n",
    "                pre_token = old_token[pos-1]\n",
    "                old_pair = (pre_token,old_token[pos])\n",
    "                new_pair = (pre_token, new_index)\n",
    "                pair_freqs_d.dec[old_pair] += count\n",
    "                pair_freqs_d.inc[new_pair] += count\n",
    "            if pos < len(old_token)-2:\n",
    "                pos_token = old_token[pos+2]\n",
    "                old_pair = (old_token[pos+1],pos_token)\n",
    "                new_pair = (new_index, pos_token)\n",
    "                pair_freqs_d.dec[old_pair] += count\n",
    "                pair_freqs_d.inc[new_pair] += count\n",
    "    return pair_freqs_d\n",
    "\n",
    "def compute_inhereit_deltas(plan: list[TokenMergePlan]) -> PairInhereitDelta:\n",
    "    pair_inhereit_d = PairInhereitDelta()\n",
    "    for item in plan:\n",
    "        if len(item.old_token) > 1:\n",
    "            for old_pair in zip(item.old_token,item.old_token[1:]):\n",
    "                pair_inhereit_d.remove[old_pair].add(item.old_token)\n",
    "        if len(item.new_token) > 1:\n",
    "            for new_pair in zip(item.new_token,item.new_token[1:]):\n",
    "                pair_inhereit_d.add[new_pair].add(item.new_token)\n",
    "    return pair_inhereit_d \n",
    "\n",
    "def exclude_pair_from_dict(d: dict, pair: Pair):\n",
    "    new_d = d.copy()\n",
    "    del new_d[pair]\n",
    "    return new_d\n",
    "\n",
    "def update_pair_freqs(pair_freqs: dict[Pair, int], pair_freqs_d: PairFreqsDelta):\n",
    "    new_pair_freqs = pair_freqs.copy()\n",
    "    for key, value in pair_freqs_d.dec.items():\n",
    "        new_pair_freqs = remove_or_decrement_pair(new_pair_freqs, key, value)\n",
    "    for key, value in pair_freqs_d.inc.items():\n",
    "        new_pair_freqs[key]+=value\n",
    "    return new_pair_freqs\n",
    "\n",
    "def update_pair_inhereit(pair_inhereit: dict[Pair, set[Encoded_Token]], pair_inhereit_d: PairInhereitDelta):\n",
    "    new_pair_inhereit = pair_inhereit.copy()\n",
    "    for key, value in pair_inhereit_d.remove.items():\n",
    "        new_pair_inhereit[key] -= value\n",
    "    for key, value in pair_inhereit_d.add.items():\n",
    "        new_pair_inhereit[key] = new_pair_inhereit[key] | value\n",
    "    return new_pair_inhereit\n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    vocab = build_initial_vocab(special_tokens)\n",
    "    vocab_len = len(vocab)\n",
    "    merges = []\n",
    "\n",
    "    string = read_text_file(input_path)\n",
    "    string_list = split_by_special_tokens(string,special_tokens)\n",
    "    token_count = count_tokens(string_list)\n",
    "    encoded_token_freqs = encode_and_count_tokens(token_count)\n",
    "    \n",
    "    pair_freqs = get_byte_pairs(encoded_token_freqs)\n",
    "    pair_inhereit = get_byte_pairs_inhereit(encoded_token_freqs)\n",
    "\n",
    "    while vocab_len < vocab_size:\n",
    "        pair = select_merge_pair(pair_freqs, vocab)\n",
    "        index1, index2 = pair\n",
    "        new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "        new_index = vocab_len\n",
    "        vocab[new_index] = new_token\n",
    "        merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "        tok_need_update = pair_inhereit[pair]\n",
    "        plan = build_merge_plan(tok_need_update, encoded_token_freqs, pair, new_index)\n",
    "        encoded_token_freqs = update_encoded_token_freqs(plan, encoded_token_freqs)\n",
    "        pair_freqs_d = compute_freqs_deltas(plan, new_index)\n",
    "        pair_freqs = exclude_pair_from_dict(pair_freqs, pair)\n",
    "        pair_freqs = update_pair_freqs(pair_freqs, pair_freqs_d)\n",
    "        pair_inhereit_d = compute_inhereit_deltas(plan)\n",
    "        pair_inhereit = exclude_pair_from_dict(pair_inhereit, pair)\n",
    "        pair_inhereit = update_pair_inhereit(pair_inhereit, pair_inhereit_d)\n",
    "        vocab_len+=1\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193e8de",
   "metadata": {},
   "source": [
    "## Version 王少东"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "    input_path: str | os.PathLike = \"data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    vocab_size: int = 1000,\n",
    "    special_tokens: list[str] = [\"<|endoftext|>\", \"<|startoftext|>\"],\n",
    ") -> tuple[Vocab, list[BytePair]]:\n",
    "    \"\"\"\n",
    "    Learn BPE merges from corpus:\n",
    "    - Initialize vocab with all single bytes (0..255) plus special tokens appended.\n",
    "    - Pretokenize and build word frequency counts.\n",
    "    - Repeatedly count adjacent pair frequencies, select the most frequent pair\n",
    "      (break ties lexicographically), apply merge, and continue until the\n",
    "      requested size is reached.\n",
    "    - Return (id_to_bytes, merges).\n",
    "    Keep this simple; implement the details yourself.\n",
    "    \"\"\"\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    for token in special_tokens:\n",
    "        vocab[len(vocab)] = token.encode(\"utf-8\")\n",
    "\n",
    "    # Pattern for GPT-2 style pretokenization\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    # Pretokenize and build word frequency counts\n",
    "    # Chunked pretokenization (serial), align to the required split token\n",
    "    split_token_bytes = b\"<|endoftext|>\"\n",
    "    with open(input_path, \"rb\") as fbin:\n",
    "        num_chunks = 100 # max(1, os.cpu_count())\n",
    "        boundaries = find_chunk_boundaries(fbin, num_chunks, split_token_bytes)\n",
    "    spans = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "    pretokenized_counter = Counter()\n",
    "    if spans:\n",
    "        cpu = max(1, os.cpu_count() - 2)\n",
    "        workers = min(len(spans), max(1, cpu))\n",
    "        if workers > 1:\n",
    "            with mp.Pool(processes=workers) as pool:\n",
    "                parts = pool.starmap(\n",
    "                    count_chunk,\n",
    "                    [(s, e, input_path, PAT, special_tokens) for s, e in spans],\n",
    "                )\n",
    "            for c in parts:\n",
    "                pretokenized_counter.update(c)\n",
    "        else:\n",
    "            # Single span or single worker fallback\n",
    "            for s, e in spans:\n",
    "                pretokenized_counter.update(\n",
    "                    count_chunk(s, e, input_path, PAT, special_tokens)\n",
    "                )\n",
    "\n",
    "    pair_counts = Counter()\n",
    "    pair_index = defaultdict(set)  # pair -> words (token tuples) that contain the pair\n",
    "    for word_seq, freq in pretokenized_counter.items():\n",
    "        if len(word_seq) < 2:\n",
    "            continue\n",
    "        for pair in zip(word_seq, word_seq[1:]):\n",
    "            pair_counts[pair] += freq\n",
    "            pair_index[pair].add(word_seq)\n",
    "\n",
    "    merges = []\n",
    "    while len(vocab) < vocab_size:\n",
    "        # pair_counts = Counter()\n",
    "        # for word_seq, freq in pretokenized_counter.items():\n",
    "        #     if len(word_seq) < 2:\n",
    "        #         continue\n",
    "        #     for i in range(len(word_seq) - 1):\n",
    "        #         pair_counts[(word_seq[i], word_seq[i + 1])] += freq\n",
    "\n",
    "        if not pair_counts:\n",
    "            break\n",
    "\n",
    "        # Select most frequent pair; break ties by lexicographically greatest pair\n",
    "        max_count = max(pair_counts.values())\n",
    "        candidates = [pair for pair, cnt in pair_counts.items() if cnt == max_count]\n",
    "        best_pair = max(candidates)\n",
    "\n",
    "        # Record merge and add merged token to vocab\n",
    "        merges.append(best_pair)\n",
    "        merged_token = best_pair[0] + best_pair[1]\n",
    "        vocab[len(vocab)] = merged_token\n",
    "\n",
    "        # Find all word sequences that contain the best pair\n",
    "        affected_words = pair_index.pop(best_pair, set())\n",
    "        if not affected_words:\n",
    "            continue\n",
    "\n",
    "        updates = {}\n",
    "        for word_seq in affected_words:\n",
    "            freq = pretokenized_counter.pop(word_seq, 0)\n",
    "            if freq == 0:\n",
    "                continue\n",
    "            # remove old pair contribution\n",
    "            for pair in zip(word_seq, word_seq[1:]):\n",
    "                pair_counts[pair] -= freq\n",
    "                if pair_counts[pair] <= 0:\n",
    "                    pair_counts.pop(pair, None)\n",
    "                pair_index[pair].discard(word_seq)\n",
    "            # merge occurrences of best_pair in word_seq\n",
    "            merged_seq = []\n",
    "            i = 0\n",
    "            while i < len(word_seq):\n",
    "                if (\n",
    "                    i + 1 < len(word_seq)\n",
    "                    and word_seq[i] == best_pair[0]\n",
    "                    and word_seq[i + 1] == best_pair[1]\n",
    "                ):\n",
    "                    merged_seq.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    merged_seq.append(word_seq[i])\n",
    "                    i += 1\n",
    "            merged_seq = tuple(merged_seq)\n",
    "            updates[merged_seq] = updates.get(merged_seq, 0) + freq\n",
    "\n",
    "        for w_new, freq in updates.items():\n",
    "            prev_freq = pretokenized_counter.get(w_new, 0)\n",
    "            for pair in zip(w_new, w_new[1:]):\n",
    "                pair_counts[pair] += freq\n",
    "                pair_index[pair].add(w_new)\n",
    "            pretokenized_counter[w_new] = freq + prev_freq\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7a435",
   "metadata": {},
   "source": [
    "## 个人优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7660b",
   "metadata": {},
   "source": [
    "## 对比最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe551196",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'\n",
    "vocab_size = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bcccfe70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'h', b'e'),\n",
       " (b' ', b'w'),\n",
       " (b' w', b'he'),\n",
       " (b' ', b't'),\n",
       " (b'r', b'e'),\n",
       " (b'i', b's'),\n",
       " (b' whe', b're'),\n",
       " (b' whe', b'n'),\n",
       " (b' t', b'he'),\n",
       " (b' ', b'y'),\n",
       " (b' ', b'is'),\n",
       " (b'o', b'u'),\n",
       " (b'n', b'n'),\n",
       " (b'nn', b'e'),\n",
       " (b'nne', b'r'),\n",
       " (b'n', b'i'),\n",
       " (b'ni', b'c'),\n",
       " (b'nic', b'e'),\n",
       " (b'm', b'e'),\n",
       " (b'me', b'e'),\n",
       " (b'mee', b't'),\n",
       " (b'l', b'i'),\n",
       " (b'i', b'nner')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab, merges = train_bpe(input_path, vocab_size, special_tokens)\n",
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bb938",
   "metadata": {},
   "source": [
    "## 对比每一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0df35532",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'\n",
    "vocab_size = 280\n",
    "string = \"\"\"hi. i'm yifan li. nice to meet you. <|endoftext|>\n",
    "this the what when here where. <|endoftext|>\n",
    "where is the car. <|endoftext|>\n",
    "when is dinner\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a00f3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []\n",
    "string_list = split_string(string, special_tokens)\n",
    "word_level_counts = get_tok_counts(string_list)\n",
    "byte_level_counts = get_byte_counts(word_level_counts)\n",
    "vocab = initiate_vocab(special_tokens)\n",
    "vocab_len = len(vocab)\n",
    "pair_counts = get_pair_counts(byte_level_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2718dfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((119, 257), 4),\n",
       " ((257, 114), 3),\n",
       " ((114, 101), 3),\n",
       " ((105, 115), 3),\n",
       " ((32, 119), 3),\n",
       " ((32, 116), 3),\n",
       " ((32, 105), 3),\n",
       " ((257, 110), 2),\n",
       " ((116, 257), 2),\n",
       " ((104, 105), 2),\n",
       " ((32, 121), 2),\n",
       " ((121, 111), 1),\n",
       " ((121, 105), 1),\n",
       " ((119, 104), 1),\n",
       " ((116, 111), 1),\n",
       " ((116, 104), 1),\n",
       " ((111, 117), 1),\n",
       " ((110, 110), 1),\n",
       " ((110, 105), 1),\n",
       " ((110, 101), 1),\n",
       " ((109, 101), 1),\n",
       " ((108, 105), 1),\n",
       " ((105, 110), 1),\n",
       " ((105, 102), 1),\n",
       " ((105, 99), 1),\n",
       " ((104, 97), 1),\n",
       " ((102, 97), 1),\n",
       " ((101, 116), 1),\n",
       " ((101, 114), 1),\n",
       " ((101, 101), 1),\n",
       " ((100, 105), 1),\n",
       " ((99, 101), 1),\n",
       " ((99, 97), 1),\n",
       " ((97, 116), 1),\n",
       " ((97, 114), 1),\n",
       " ((97, 110), 1),\n",
       " ((39, 109), 1),\n",
       " ((32, 257), 1),\n",
       " ((32, 110), 1),\n",
       " ((32, 109), 1),\n",
       " ((32, 108), 1),\n",
       " ((32, 100), 1),\n",
       " ((32, 99), 1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_items = sorted(pair_counts.items(), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "sorted_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b785ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pair = find_max_pair(pair_counts, vocab)\n",
    "index1, index2 = pair\n",
    "new_token = vocab[int(index1)]+vocab[int(index2)]\n",
    "new_index = vocab_len\n",
    "byte_level_counts = update_element_counts(byte_level_counts, pair,new_index)\n",
    "pair_counts = get_pair_counts(byte_level_counts)\n",
    "merges.append((vocab[int(index1)], vocab[int(index2)]))\n",
    "vocab[new_index] = new_token\n",
    "vocab_len+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62f50d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(104, 105): 1,\n",
       " (46,): 5,\n",
       " (32, 105): 1,\n",
       " (39, 109): 1,\n",
       " (32, 121, 105, 102, 97, 110): 1,\n",
       " (32, 108, 105): 1,\n",
       " (32, 110, 105, 99, 101): 1,\n",
       " (32, 116, 111): 1,\n",
       " (32, 109, 101, 101, 116): 1,\n",
       " (32, 121, 111, 117): 1,\n",
       " (32,): 3,\n",
       " (32, 116, 104, 105, 115): 1,\n",
       " (32, 116, 257): 2,\n",
       " (32, 119, 104, 97, 116): 1,\n",
       " (32, 119, 257, 110): 2,\n",
       " (32, 257, 114, 101): 1,\n",
       " (32, 119, 257, 114, 101): 2,\n",
       " (32, 105, 115): 2,\n",
       " (32, 99, 97, 114): 1,\n",
       " (32, 100, 105, 110, 110, 101, 114): 1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_level_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ebb58",
   "metadata": {},
   "source": [
    "## 王少东的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eabdaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter, defaultdict\n",
    "import multiprocessing as mp\n",
    "import regex as re\n",
    "from typing import BinaryIO, Iterable, Iterator\n",
    "import time\n",
    "import pickle\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(\n",
    "        split_special_token, bytes\n",
    "    ), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "\n",
    "def count_chunk(\n",
    "    start: int, end: int, path: str, pat: str, specials: list[str]\n",
    ") -> Counter:\n",
    "    \"\"\"Count pre-tokens within a file slice [start, end). Minimal helper for multiprocessing.\"\"\"\n",
    "    counter = Counter()\n",
    "    with open(path, \"rb\") as fh:\n",
    "        fh.seek(start)\n",
    "        raw = fh.read(end - start)\n",
    "    text = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "    # Normalize newlines so Windows CRLF does not introduce stray \\r tokens\n",
    "    # This ensures reproducible tokenization across platforms\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\")\n",
    "    specials_set = set(specials)\n",
    "    if specials:\n",
    "        split_pat = \"|\".join(\n",
    "            re.escape(tok) for tok in specials\n",
    "        )  # escape special tokens since some have \"|\" in them\n",
    "        segments = re.split(split_pat, text)\n",
    "    else:\n",
    "        segments = [text]\n",
    "    for segment in segments:\n",
    "        if not segment:\n",
    "            continue\n",
    "        for match in re.finditer(pat, segment):\n",
    "            token_text = match.group(0)\n",
    "            token_bytes = token_text.encode(\"utf-8\")\n",
    "            seq = tuple(bytes([b]) for b in token_bytes)\n",
    "            counter[seq] += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9beef343",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "input_path = fr'./data/test.txt'\n",
    "vocab_size = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b549abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "for token in special_tokens:\n",
    "    vocab[len(vocab)] = token.encode(\"utf-8\")\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "split_token_bytes = b\"<|endoftext|>\"\n",
    "with open(input_path, \"rb\") as fbin:\n",
    "    num_chunks = 100 # max(1, os.cpu_count())\n",
    "    boundaries = find_chunk_boundaries(fbin, num_chunks, split_token_bytes)\n",
    "spans = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "pretokenized_counter = Counter()\n",
    "workers = 1\n",
    "for s, e in spans:\n",
    "    pretokenized_counter.update(\n",
    "        count_chunk(s, e, input_path, PAT, special_tokens)\n",
    "    )\n",
    "pair_counts = Counter()\n",
    "pair_index = defaultdict(set)  # pair -> words (token tuples) that contain the pair\n",
    "for word_seq, freq in pretokenized_counter.items():\n",
    "    if len(word_seq) < 2:\n",
    "        continue\n",
    "    for pair in zip(word_seq, word_seq[1:]):\n",
    "        pair_counts[pair] += freq\n",
    "        pair_index[pair].add(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "164aa425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(b'.',): 5,\n",
       "         (b' ',): 3,\n",
       "         (b'\"',): 2,\n",
       "         (b' ', b't', b'h', b'e'): 2,\n",
       "         (b' ', b'w', b'h', b'e', b'n'): 2,\n",
       "         (b' ', b'w', b'h', b'e', b'r', b'e'): 2,\n",
       "         (b' ', b'i', b's'): 2,\n",
       "         (b'h', b'i'): 1,\n",
       "         (b' ', b'i'): 1,\n",
       "         (b\"'\", b'm'): 1,\n",
       "         (b' ', b'y', b'i', b'f', b'a', b'n'): 1,\n",
       "         (b' ', b'l', b'i'): 1,\n",
       "         (b' ', b'n', b'i', b'c', b'e'): 1,\n",
       "         (b' ', b't', b'o'): 1,\n",
       "         (b' ', b'm', b'e', b'e', b't'): 1,\n",
       "         (b' ', b'y', b'o', b'u'): 1,\n",
       "         (b' ', b't', b'h', b'i', b's'): 1,\n",
       "         (b' ', b'w', b'h', b'a', b't'): 1,\n",
       "         (b' ', b'h', b'e', b'r', b'e'): 1,\n",
       "         (b' ', b'c', b'a', b'r'): 1,\n",
       "         (b' ', b'd', b'i', b'n', b'n', b'e', b'r'): 1})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokenized_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a393b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(b'h', b'e'): 7,\n",
       "         (b' ', b'w'): 5,\n",
       "         (b'w', b'h'): 5,\n",
       "         (b' ', b't'): 4,\n",
       "         (b'e', b'r'): 4,\n",
       "         (b' ', b'i'): 3,\n",
       "         (b't', b'h'): 3,\n",
       "         (b'i', b's'): 3,\n",
       "         (b'r', b'e'): 3,\n",
       "         (b'h', b'i'): 2,\n",
       "         (b' ', b'y'): 2,\n",
       "         (b'e', b'n'): 2,\n",
       "         (b\"'\", b'm'): 1,\n",
       "         (b'y', b'i'): 1,\n",
       "         (b'i', b'f'): 1,\n",
       "         (b'f', b'a'): 1,\n",
       "         (b'a', b'n'): 1,\n",
       "         (b' ', b'l'): 1,\n",
       "         (b'l', b'i'): 1,\n",
       "         (b' ', b'n'): 1,\n",
       "         (b'n', b'i'): 1,\n",
       "         (b'i', b'c'): 1,\n",
       "         (b'c', b'e'): 1,\n",
       "         (b't', b'o'): 1,\n",
       "         (b' ', b'm'): 1,\n",
       "         (b'm', b'e'): 1,\n",
       "         (b'e', b'e'): 1,\n",
       "         (b'e', b't'): 1,\n",
       "         (b'y', b'o'): 1,\n",
       "         (b'o', b'u'): 1,\n",
       "         (b'h', b'a'): 1,\n",
       "         (b'a', b't'): 1,\n",
       "         (b' ', b'h'): 1,\n",
       "         (b' ', b'c'): 1,\n",
       "         (b'c', b'a'): 1,\n",
       "         (b'a', b'r'): 1,\n",
       "         (b' ', b'd'): 1,\n",
       "         (b'd', b'i'): 1,\n",
       "         (b'i', b'n'): 1,\n",
       "         (b'n', b'n'): 1,\n",
       "         (b'n', b'e'): 1})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29391014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {(b'h', b'i'): {(b' ', b't', b'h', b'i', b's'), (b'h', b'i')},\n",
       "             (b' ', b'i'): {(b' ', b'i'), (b' ', b'i', b's')},\n",
       "             (b\"'\", b'm'): {(b\"'\", b'm')},\n",
       "             (b' ', b'y'): {(b' ', b'y', b'i', b'f', b'a', b'n'),\n",
       "              (b' ', b'y', b'o', b'u')},\n",
       "             (b'y', b'i'): {(b' ', b'y', b'i', b'f', b'a', b'n')},\n",
       "             (b'i', b'f'): {(b' ', b'y', b'i', b'f', b'a', b'n')},\n",
       "             (b'f', b'a'): {(b' ', b'y', b'i', b'f', b'a', b'n')},\n",
       "             (b'a', b'n'): {(b' ', b'y', b'i', b'f', b'a', b'n')},\n",
       "             (b' ', b'l'): {(b' ', b'l', b'i')},\n",
       "             (b'l', b'i'): {(b' ', b'l', b'i')},\n",
       "             (b' ', b'n'): {(b' ', b'n', b'i', b'c', b'e')},\n",
       "             (b'n', b'i'): {(b' ', b'n', b'i', b'c', b'e')},\n",
       "             (b'i', b'c'): {(b' ', b'n', b'i', b'c', b'e')},\n",
       "             (b'c', b'e'): {(b' ', b'n', b'i', b'c', b'e')},\n",
       "             (b' ', b't'): {(b' ', b't', b'h', b'e'),\n",
       "              (b' ', b't', b'h', b'i', b's'),\n",
       "              (b' ', b't', b'o')},\n",
       "             (b't', b'o'): {(b' ', b't', b'o')},\n",
       "             (b' ', b'm'): {(b' ', b'm', b'e', b'e', b't')},\n",
       "             (b'm', b'e'): {(b' ', b'm', b'e', b'e', b't')},\n",
       "             (b'e', b'e'): {(b' ', b'm', b'e', b'e', b't')},\n",
       "             (b'e', b't'): {(b' ', b'm', b'e', b'e', b't')},\n",
       "             (b'y', b'o'): {(b' ', b'y', b'o', b'u')},\n",
       "             (b'o', b'u'): {(b' ', b'y', b'o', b'u')},\n",
       "             (b't', b'h'): {(b' ', b't', b'h', b'e'),\n",
       "              (b' ', b't', b'h', b'i', b's')},\n",
       "             (b'i', b's'): {(b' ', b'i', b's'),\n",
       "              (b' ', b't', b'h', b'i', b's')},\n",
       "             (b'h', b'e'): {(b' ', b'h', b'e', b'r', b'e'),\n",
       "              (b' ', b't', b'h', b'e'),\n",
       "              (b' ', b'w', b'h', b'e', b'n'),\n",
       "              (b' ', b'w', b'h', b'e', b'r', b'e')},\n",
       "             (b' ', b'w'): {(b' ', b'w', b'h', b'a', b't'),\n",
       "              (b' ', b'w', b'h', b'e', b'n'),\n",
       "              (b' ', b'w', b'h', b'e', b'r', b'e')},\n",
       "             (b'w', b'h'): {(b' ', b'w', b'h', b'a', b't'),\n",
       "              (b' ', b'w', b'h', b'e', b'n'),\n",
       "              (b' ', b'w', b'h', b'e', b'r', b'e')},\n",
       "             (b'h', b'a'): {(b' ', b'w', b'h', b'a', b't')},\n",
       "             (b'a', b't'): {(b' ', b'w', b'h', b'a', b't')},\n",
       "             (b'e', b'n'): {(b' ', b'w', b'h', b'e', b'n')},\n",
       "             (b' ', b'h'): {(b' ', b'h', b'e', b'r', b'e')},\n",
       "             (b'e', b'r'): {(b' ', b'd', b'i', b'n', b'n', b'e', b'r'),\n",
       "              (b' ', b'h', b'e', b'r', b'e'),\n",
       "              (b' ', b'w', b'h', b'e', b'r', b'e')},\n",
       "             (b'r', b'e'): {(b' ', b'h', b'e', b'r', b'e'),\n",
       "              (b' ', b'w', b'h', b'e', b'r', b'e')},\n",
       "             (b' ', b'c'): {(b' ', b'c', b'a', b'r')},\n",
       "             (b'c', b'a'): {(b' ', b'c', b'a', b'r')},\n",
       "             (b'a', b'r'): {(b' ', b'c', b'a', b'r')},\n",
       "             (b' ', b'd'): {(b' ', b'd', b'i', b'n', b'n', b'e', b'r')},\n",
       "             (b'd', b'i'): {(b' ', b'd', b'i', b'n', b'n', b'e', b'r')},\n",
       "             (b'i', b'n'): {(b' ', b'd', b'i', b'n', b'n', b'e', b'r')},\n",
       "             (b'n', b'n'): {(b' ', b'd', b'i', b'n', b'n', b'e', b'r')},\n",
       "             (b'n', b'e'): {(b' ', b'd', b'i', b'n', b'n', b'e', b'r')}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3027d3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b' ', b'h', b'e', b'r', b'e'),\n",
       " (b' ', b't', b'h', b'e'),\n",
       " (b' ', b'w', b'h', b'e', b'n'),\n",
       " (b' ', b'w', b'h', b'e', b'r', b'e')}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges = []\n",
    "\n",
    "\n",
    "max_count = max(pair_counts.values())\n",
    "candidates = [pair for pair, cnt in pair_counts.items() if cnt == max_count]\n",
    "best_pair = max(candidates)\n",
    "# Record merge and add merged token to vocab\n",
    "merges.append(best_pair)\n",
    "merged_token = best_pair[0] + best_pair[1]\n",
    "vocab[len(vocab)] = merged_token\n",
    "\n",
    "affected_words = pair_index.pop(best_pair, set())\n",
    "affected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all word sequences that contain the best pair\n",
    "affected_words = pair_index.pop(best_pair, set())\n",
    "if not affected_words:\n",
    "    continue\n",
    "\n",
    "updates = {}\n",
    "for word_seq in affected_words:\n",
    "    freq = pretokenized_counter.pop(word_seq, 0)\n",
    "    if freq == 0:\n",
    "        continue\n",
    "    # remove old pair contribution\n",
    "    for pair in zip(word_seq, word_seq[1:]):\n",
    "        pair_counts[pair] -= freq\n",
    "        if pair_counts[pair] <= 0:\n",
    "            pair_counts.pop(pair, None)\n",
    "        pair_index[pair].discard(word_seq)\n",
    "    # merge occurrences of best_pair in word_seq\n",
    "    merged_seq = []\n",
    "    i = 0\n",
    "    while i < len(word_seq):\n",
    "        if (\n",
    "            i + 1 < len(word_seq)\n",
    "            and word_seq[i] == best_pair[0]\n",
    "            and word_seq[i + 1] == best_pair[1]\n",
    "        ):\n",
    "            merged_seq.append(merged_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_seq.append(word_seq[i])\n",
    "            i += 1\n",
    "    merged_seq = tuple(merged_seq)\n",
    "    updates[merged_seq] = updates.get(merged_seq, 0) + freq\n",
    "\n",
    "for w_new, freq in updates.items():\n",
    "    prev_freq = pretokenized_counter.get(w_new, 0)\n",
    "    for pair in zip(w_new, w_new[1:]):\n",
    "        pair_counts[pair] += freq\n",
    "        pair_index[pair].add(w_new)\n",
    "    pretokenized_counter[w_new] = freq + prev_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "087a4594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretokenized_counter[word_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not affected_words:\n",
    "    continue\n",
    "\n",
    "updates = {}\n",
    "for word_seq in affected_words:\n",
    "    freq = pretokenized_counter.pop(word_seq, 0)\n",
    "    if freq == 0:\n",
    "        continue\n",
    "    # remove old pair contribution\n",
    "    for pair in zip(word_seq, word_seq[1:]):\n",
    "        pair_counts[pair] -= freq\n",
    "        if pair_counts[pair] <= 0:\n",
    "            pair_counts.pop(pair, None)\n",
    "        pair_index[pair].discard(word_seq)\n",
    "    # merge occurrences of best_pair in word_seq\n",
    "    merged_seq = []\n",
    "    i = 0\n",
    "    while i < len(word_seq):\n",
    "        if (\n",
    "            i + 1 < len(word_seq)\n",
    "            and word_seq[i] == best_pair[0]\n",
    "            and word_seq[i + 1] == best_pair[1]\n",
    "        ):\n",
    "            merged_seq.append(merged_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_seq.append(word_seq[i])\n",
    "            i += 1\n",
    "    merged_seq = tuple(merged_seq)\n",
    "    updates[merged_seq] = updates.get(merged_seq, 0) + freq\n",
    "\n",
    "for w_new, freq in updates.items():\n",
    "    prev_freq = pretokenized_counter.get(w_new, 0)\n",
    "    for pair in zip(w_new, w_new[1:]):\n",
    "        pair_counts[pair] += freq\n",
    "        pair_index[pair].add(w_new)\n",
    "    pretokenized_counter[w_new] = freq + prev_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ad74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanford",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
